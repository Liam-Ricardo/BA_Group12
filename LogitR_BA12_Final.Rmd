---
title: "BA_12_LogitR"
output: html_document
date: "2025-03-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


destination_path = "D:/Netherlands/TU DELFT/Q3 2024/Business Analytics/R/trending_yt_videos_113_countries_model_training.csv"

df <- read.csv(destination_path)
print(names(df))

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:



```{r }

# Load necessary libraries
library(dplyr)
library(ggplot2)

set.seed(123)

# Add row ID to ensure clean stratified sampling
df$row_id <- 1:nrow(df)

# Stratified sampling: 90% train per class
train_indices <- df %>%
  group_by(trending) %>%
  slice_sample(prop = 0.9) %>%
  pull(row_id)

# Create train and validation datasets
train_data <- df %>% filter(row_id %in% train_indices)
val_data <- df %>% filter(!(row_id %in% train_indices))

# Remove row_id
train_data <- select(train_data, -row_id)
val_data <- select(val_data, -row_id)

# Shuffle (optional)
train_data <- train_data[sample(nrow(train_data)), ]
val_data <- val_data[sample(nrow(val_data)), ]

# Check split sizes and distributions
message("Train size:", nrow(train_data))
message("Validation size:", nrow(val_data))

message("Train trending distribution:")
print(table(train_data$trending))

message("Validation trending distribution:")
print(table(val_data$trending))

# Train logistic regression model using features NOT in label creation
logit_model <- glm(trending ~ title_score + video_tag_score +  engagement_score + comment_score + video_age,
                   data = train_data, family = "binomial")

# Summary
summary(logit_model)

# Evaluation function
evaluate_model <- function(model, data, dataset_name = "Validation") {
  predictions <- predict(model, newdata = data, type = "response")
  predicted_classes <- factor(ifelse(predictions > 0.5, 1, 0), levels = c(0, 1))
  actual_classes <- data$trending
  
  conf_matrix <- table(Predicted = predicted_classes, Actual = actual_classes)
  
  message(paste0("\nConfusion Matrix (", dataset_name, " Set):"))
  print(conf_matrix)
  
  # Metrics
  total <- sum(conf_matrix)
  accuracy <- if (total == 0) NA else sum(diag(conf_matrix)) / total
  precision <- if (sum(conf_matrix["1", ]) == 0) NA else conf_matrix["1", "1"] / sum(conf_matrix["1", ])
  recall <- if (sum(conf_matrix[, "1"]) == 0) NA else conf_matrix["1", "1"] / sum(conf_matrix[, "1"])
  f1_score <- if (is.na(precision) || is.na(recall) || (precision + recall) == 0) NA else (2 * precision * recall) / (precision + recall)
  
  message(paste("\nEvaluation Metrics (", dataset_name, " Set):", sep = ""))
  message(paste("Accuracy :", round(accuracy, 4)))
  message(paste("Precision:", round(precision, 4)))
  message(paste("Recall   :", round(recall, 4)))
  message(paste("F1 Score :", round(f1_score, 4)))
}

# Evaluate on train and validation sets
evaluate_model(logit_model, train_data, dataset_name = "Training")
evaluate_model(logit_model, val_data, dataset_name = "Validation")

# Feature importance plot
feature_importance <- abs(coef(logit_model)[-1]) # Exclude intercept
feature_df <- data.frame(Feature = names(feature_importance), Importance = feature_importance)

ggplot(feature_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  ggtitle("Feature Importance in Trending Prediction") +
  xlab("Feature") +
  ylab("Importance")
```


```{r}


```

