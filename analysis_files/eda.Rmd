---
title: "eda"
output: html_document
---

# Exploratory Data Analysis

As usual, we start with importing the required libraries and the dataset.

```{r import libraries, include=FALSE}
# Load necessary libraries

library(ggplot2)
library(dplyr)
library(tidyr)
library(wordcloud2)
library(tm)
library(lubridate)  # For time aggregation

library(stringr)
library(textTinyR)
library(tm)
library(hunspell)
library(SnowballC)
```

```{r import data}

data_filepath <- "C:/Users/sabes/Desktop/Group Assignment/trending_yt_videos_113_countries_cleaned.csv"

# cleaned_data <- read.csv(data_filepath)
cleaned_data <- read.csv(data_filepath, stringsAsFactors = FALSE)
```

## 1. Univariate Analysis

The first step in EDA is "`Univariate analysis`" - in a nutshell - means that analysing each variable separately. This helps us to have an in-depth understanding of all the variables. It is an important step because - to put it in terms of a car - ***without knowing what each screw and bolt does, you cannot possibly build a car or repair it.***

### 1.1. Numerical Columns

Let's start by defining some `function(s)` that will take a dataframe as input, check for the type of column and if its the intended type (eg `int` or `chr`) for that function, it will analyse that column and print the respective results.

-   For `numerical columns` the function will calculate the `mean, median, mode, skweness, kutosis, standard deviation,` and `quartile values`.

-   For `categorical columns` the function will check for the `frequency counts` and plots a `bar graph`

```{r}
analyze_numerical <- function(df) {
  numerical_cols <- names(df)[sapply(df, is.numeric)]
  
  for (col in numerical_cols) {
    cat("\nColumn Name: ", col, "\n")
    print(summary(df[[col]]))
    cat("Skewness:", e1071::skewness(df[[col]], na.rm = TRUE), "\n")
    cat("Kurtosis:", e1071::kurtosis(df[[col]], na.rm = TRUE), "\n")
    print("-------------------------------------------------------")
    
    # Visualizations
    pic_1 <- ggplot(df, aes(x = .data[[col]])) + 
      geom_histogram(aes(y = ..density..), bins = 30, fill = "blue", alpha = 0.5) +
      geom_density(color = "red") +
      ggtitle(paste("Histogram of", col))
    
    pic_1 <- ggplot(df, aes(y = .data[[col]])) + 
      geom_boxplot(fill = "orange", alpha = 0.5) +
      ggtitle(paste("Boxplot of", col))
    
    print(pic_1)
    print(pic_1)
  }
}
```

```{r}
analyze_categorical <- function(df) {
  categorical_cols <- names(df)[sapply(df, is.character) | sapply(df, is.factor)]
  
  for (col in categorical_cols) {
    cat("\nColumn Name: ", col, "\n")
    print(table(df[[col]]))
    
    # Visualization
    pic <- ggplot(df, aes(x = .data[[col]])) + 
      geom_bar(fill = "blue", alpha = 0.5) +
      ggtitle(paste("Bar Plot of", col)) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    print(pic)
    print("-------------------------------------------------------")
  }
}
```

```{r}
analyze_numerical(cleaned_data)
```

```{r}
analyze_categorical(cleaned_data)
```

```{r}

# a sample code from chatGPT to conduct Trend Analysis

#trend_analysis <- function(df, date_col, num_col, time_unit = "month") {
#  df <- df %>%
#    mutate(aggregated_date = floor_date(.data[[date_col]], unit = time_unit)) %>%# Aggregate by time unit
#    group_by(aggregated_date) %>%
#    summarise(mean_value = mean(.data[[num_col]], na.rm = TRUE), .groups = "drop")
  
#  ggplot(df, aes(x = aggregated_date, y = mean_value)) +
#    geom_line(color = "blue") +
#    geom_point(color = "red") +
#    geom_smooth(method = "loess", se = FALSE, color = "green") +
#    labs(title = paste("Trend Analysis of", num_col, "over Time"),
#         x = date_col,
#         y = paste("Average", num_col)) +
#    theme_minimal()
#}

# Example Usage
# df <- data.frame(date = seq(as.Date("2024-01-01"), 
#                             as.Date("2024-12-31"), 
#                             by = "day"),
#                  value = rnorm(366, mean = 100, sd = 20))

# trend_analysis(df, "date", "value", "month")
```

### 1.2. Text Columns

-   For `text data` the function will calculate most commonly appearing words in the `title and video_tags` columns in the `top 10%` of the videos.

-   Top 10% - based on `daily_rank, daily_movement and weekly_movement.` And merging the resultant vectors into one, while removing the common words between the 2 techniques and correcting the spelling using `hunspell` library to ensure we have the words that are meaningful.

-   Further in the analysis, we can use this result as a feature to train the model - to be one of the factors that determine the trendliness of the video.

```{r}

stopwords_list <- stopwords("en")  # Define globally

tokenize <- function(text) {
  words <- unlist(str_extract_all(tolower(text), "\\b\\w+\\b"))
  words <- words[!(words %in% stopwords_list) & nchar(words) > 4 & !grepl("^\\d+$", words)]
  return(words)
}

get_top_words <- function(df, ranking_type) {
  if (tolower(ranking_type) == "rank") {
    threshold <- quantile(df$daily_rank, 0.1, na.rm = TRUE)
    top_videos <- df %>% filter(daily_rank <= threshold)
  } else if (tolower(ranking_type) == "movement") {
    threshold_daily <- quantile(df$daily_movement, 0.1, na.rm = TRUE)
    threshold_weekly <- quantile(df$weekly_movement, 0.1, na.rm = TRUE)
    top_videos <- df %>% filter(daily_movement <= threshold_daily | weekly_movement <= threshold_weekly)
  } else {
    stop("Invalid ranking_type. Use 'rank' or 'movement'.")
  }
  
  # Ignore NA values
  top_videos <- top_videos %>% drop_na(title, video_tags)
  
  # Process title words
  title_words <- unlist(strsplit(top_videos$title, " ")) %>% tokenize()
  title_word_counts <- sort(table(title_words), decreasing = TRUE)
  
  # Process video tags words
  tag_words <- unlist(strsplit(top_videos$video_tags, " ")) %>% tokenize()
  tag_word_counts <- sort(table(tag_words), decreasing = TRUE)
  
  return(list(
    names(title_word_counts)[1:25],
    names(tag_word_counts)[1:25]
  ))
}

common_words_list_rank <- get_top_words(cleaned_data, "rank")
common_words_list_movement <- get_top_words(cleaned_data, "movement")
```

```{r}
normalize_word <- function(word) {
  word <- tolower(word)  # Convert to lowercase
  word <- gsub("[- ]", "", word)  # Remove hyphens and spaces
  return(word)
}

stem_word <- function(word) {
  return(SnowballC::wordStem(word, language = "en"))
}

correct_spelling <- function(words) {
  words <- unique(sapply(words, normalize_word))  # Normalize words first
  corrected_words <- unique(unlist(sapply(words, function(word) {
    suggestions <- hunspell_suggest(word)
    if (length(suggestions) > 0) {
      return(suggestions[[1]][1])  # Pick the first suggestion
    } else {
      return(word)  # Keep original word if no suggestion available
    }
  })))
  return(sort(corrected_words))
}


merge_word_tuples <- function(tuple1, tuple2) {
  title_words_combined <- unique(c(tuple1[[1]], tuple2[[1]]))
  tag_words_combined <- unique(c(tuple1[[2]], tuple2[[2]]))
  
  return(list(
    correct_spelling(title_words_combined),
    correct_spelling(tag_words_combined)
  ))
}
```

```{r}
final_results <- merge_word_tuples(common_words_list_rank, common_words_list_movement)
final_title_words <- final_results[[1]]
final_tag_words <- final_results[[2]]

print(paste("Final Title:", length(final_title_words), toString(final_title_words)))
print(paste("Final Tags:", length(final_tag_words), toString(final_tag_words)))
```

Finally, after God knows how many tries and annoying ChatGpt to help me migrate the codesðŸ˜’, R was able to replicate the Python function to get the most commonly used words in the top 10% videos, with almost the same accuracy. Now we have 2 lists of words that we can use to introduce a synthetic column that in turn can be used as a feature to train the model.

Logic of `title_score` and `video_tags_score` columns -\> Take the value in the cell (say 1st cell of title (or video_tags) column - "*Highlights Real Madrid vs Barcelona 3-1...*") and run the words separately through the list of words in `final_title_words` (or `final_tag_words`) and increase the count by 1 every time the word in the cell matches a word in the list. Example: The title "*Highlights Real Madrid vs Barcelona 3-1..."* will get a score of **3.**

```{r}
# Function to calculate title_score
calculate_title_score <- function(title, word_list) {
  words <- tokenize(title)
  matches <- sapply(words, function(word) {
    any(grepl(paste0("\\b", word, "\\b"), word_list, ignore.case = TRUE))
  })
  # Ensure matches is a numeric vector (TRUE -> 1, FALSE -> 0)
  score <- sum(as.numeric(matches))
  return(as.integer(score))
}
```

```{r}
# Apply function to create title_score column using vapply for type safety
cleaned_data <- cleaned_data %>%
  mutate(title_score = vapply(title, calculate_title_score, FUN.VALUE = integer(1), word_list = final_title_words))
```

```{r}
cleaned_data <- cleaned_data %>%
  mutate(video_tag_score = vapply(video_tags, calculate_title_score, FUN.VALUE = integer(1), word_list = final_tag_words))
```

This functions have added a column with the intended purpose. Let's try to feed that into the model and see if its improving the performance.
