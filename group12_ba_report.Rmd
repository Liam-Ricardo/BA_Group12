---
title: "group12_ba_report"
output: html_document
---

# MOT122A - Business Analytics Group Assignment

## Overview

### Problem Statement

Capturing the audience's attention in the current digital landscape is a substantial challenge with the endless content at users' fingertips. Every minute, 1,800 YouTube videos are posted, amounting to a 360 hour increase in available content on the platform [[1](https://seo.ai/blog/how-many-videos-are-on-youtube)]. As a marketing professional, how do you ensure your video stands out? This challenge is further exacerbated by humans' ever-decreasing average attention span which, in 2024, was measured at just 8 seconds [[2](https://devrix.com/tutorial/user-attention-span/#:~:text=The%20average%20user%20attention%20span,8%20seconds%20in%20recent%20years)].

Attracting the audience's attention is challenging enough; sustaining it is yet another. Both aspects are critical for successful marketing campaigns which is why industry professionals such as content creators and product/service marketing teams need to know their audiences, know what keeps them interested, what drives them away, and what levers they can pull to increase their reach. This is where machine learning comes in.

### Task

Develop a machine learning model to classify a video's trendiness given the video metadata, engagement statistics, and temporal rankings.

### Application

This machine learning model can be leveraged by marketing professionals to predict the likelihood that a given YouTube video will become trendy. This information informs tactical choices in the development process, enabling the professional to optimize video content and publishing strategies to increase the chance of the video becoming trendy. A trendy video subsequently increases the video's number of impressions which is a typical KPI (key performance indicator) among marketing teams. This, in turn, usually influences engagement metrics, another typical KPI among marketing campaigns and industry professionals.

### Dataset

We are using the dataset from Kaggle - [Trending Youtube Video Statistics (113 Countries)](https://www.kaggle.com/datasets/asaniczka/trending-youtube-videos-113-countries). Top 50 latest trending videos on YouTube across 113 countries. With **daily updates**, this dataset provides comprehensive information about the top trending videos, including daily rankings, movement trends, view counts, likes, comments, and more. We are using a version of the dataset downloaded on 20 February 2025.

------------------------------------------------------------------------

## Data Cleaning

### 1. Importing the required libraries

Only use this cells to import all the required libraries. DO NOT import the libraries in any other cells below, it helps with maintaining the code properly and enhances the readability of the code.

```{r import_libraries, include=FALSE}
library(dplyr) 
library(tidyr) 
library(stringr) 
library(lubridate)
library(ggplot2)
library(textTinyR)
library(tm)
library(hunspell)
library(SnowballC)
library(factoextra)
library(C50)
library(gmodels)
library(Hmisc)
library(randomForest)
library(rsample)
library(caret)
library(tidyverse)

set.seed(123)
```

Now that we have all the required libraries imported, let's dive into analyzing the dataset and train a ML model to classify the video as a Trending video or not. First, let's start by importing the dataset and cleaning it.

#### Import Dataset

This cell imports the raw data. Since the data is too large for `GitHub` we can only use the data in our local system. So, please make sure you update the `filepath` with your own path before proceeding further in the notebook.

```{r import_dataset}
# Update this variable with the your own filepath
data_filepath <-"./dataset/trending_yt_videos_113_countries.csv"

#Import dataset
yt_df <- read.csv(data_filepath)
```

> *Tip: You can run all the cells at once to get the final model (or) you can import different versions of the dataset at particular sections and run the remainder of the codes. The latter is suggested, because of RStudio's limited memory capacity. The places where you can import the already created dataset will be marked with `import dataset (optional)` text.*

### 2. Inspecting the Raw Data

We are just performing an initial check on the raw data. We do not manipulate any data points here as the main aim is to understand the raw data. First, let's look at the structure of the data.

```{r str_dataset}
# Structure of the dataset 
str(yt_df)
```

As we can see, we have a lot of `chr` datatypes and a few of `int` data type. Next, we will check the summary statistics (like mean, median, min and max of the numerical columns and length, class, mode for non-numerical columns) *[Tip: Check `console` output for better readability]*

```{r summary_dataset}
# Summary statistics 
summary(yt_df)
```

Now that we have a glimpse of what raw data we are dealing with, let's visualize it in the table format to soak in its beauty. *[Tip: Use the arrows to navigate to other columns]*

```{r head_dataset}
# First few rows 
head(yt_df)
```

Now, it's time to understand how many rows and columns we have in the raw data. Trust us, this step is as important as every other steps. It might not seem like an great insight right now, but if and when we have to merge this dataset with another, the original dimensions will come in handy. And after data pre-processing, later on in this report, we will compare the final model training dataset with the raw data it as this is a good practice.

```{r dimensions_dataset}
# Dimensions of the dataset 
dim(yt_df)
```

```{r numerical vs non-numerical}
# Create a classification of columns as numerical or non-numerical
column_types <- sapply(yt_df, is.numeric)

# Extract numerical and non-numerical column names
numerical_columns <- names(column_types[column_types == TRUE])
non_numerical_columns <- names(column_types[column_types == FALSE])

# Print grouped column names
cat("Numerical Columns:\n")
cat(numerical_columns, sep = "\n")

cat("\nNon-Numerical Columns:\n")
cat(non_numerical_columns, sep = "\n")
```

With 18 columns, not all of them will be relevant for predicting whether a YouTube video becomes trendy or not. But at first-sight the following columns are worth diving deeper into:

[daily_rank:]{.underline} This is the position a video has at a given day. The ranking ranges from 1-50, in which 1 is the highest rank and 50 the lowest. The ranking is done of all videos within a country (a total of 113 countries in the dataset).

[daily_movement:]{.underline} This shows how much movement in ranking a video had at a given day. Movement is bounded between -49 to +49.

[weekly_moment:]{.underline} Similar to `daily_movement` but then movement measured within a single week. Again bounded between -49 to +49.

[view_count:]{.underline} The amount of views a video has had, which is one of the primary KPIs for marketing campaigns and content creators.

Most industry professionals are also very interested in the audience's interaction with the content and engagement metrics track this relationship. Our dataset contains a few columns which are well-suited to be part of engagement metrics. Here they are:

[like_count:]{.underline} This is the total likes a video has received. It is also an important KPI and might become part of an engagement metric variable later on in the analysis.

[comment_count:]{.underline} Similar to `like_count` but then done with regards to comments per video.

**Summary of insights gained:**

1.  We have 2,720,499 rows and 18 columns in our dataset

2.  12 `char` (non-numerical) columns and 6 `int` (numerical) columns

3.  The `int` columns seems to be properly formatted (i.e. without any `str` in between)

4.  At first sight, interesting columns for predicting trendiness are `daily_rank`, `daily_movement`, `weekly_movement`, and `view_count`. For engagement metrics columns `like_count` and `comment_count` are relevant.

### 3. Data Pre-Processing

In this step, we will clean the raw data as much as possible, so that it is Exploratory Data Analysis (EDA) ready. First we start with making sure all the column have appropriate datatypes (like `date` type values in `snapshot_date` column).

#### 3.1. Standardizing Column Names and Removing Duplicate Values

Let's define a user-defined function (`udf`) to do the initial cleaning of the dataset. The function will:

1.  Standardize the column names (lower_case)

2.  Drop duplicate rows

3.  Drop unnecessary columns: [`channel_name`, `country`, `description`, `thumbnail_url`, `kind`, `video_id`, `channel_id`, `language]`. This step was originally supposed to be at the end of EDA, but due to RStudio's memory constraints we are performing it here. Rationale is as follows:

    `kind` : Only one unique value (Youtube video).

    `country` , `language` : We are concerned with macro trends in the data, not country or language-specific trends. Further, the language column had a large number of blank/null values.

    `description` : A large number of blank/null values and converting the description column to numeric with Natural Language Processing (NLP) would be complex.

    `channel_name` , `channel_id` : Minimal contribution to trendiness.

    `thumbnail_url`, `video_id` : No contribution to trendiness.

```{r udf_std_colnames}
clean_data <- function(df){      
  # Standardize column names   
  cleaned_data <- df %>% rename_all(tolower) %>% rename_all(str_replace_all, pattern = " ", replacement = "_")      
  # Remove duplicate rows   
  df <- df %>% distinct()      
  # Dropping unnecessary columns
  df <- df %>% select(-c(channel_name, country, description, thumbnail_url, kind, video_id, channel_id, langauge))
  
  return(df) 
}
```

*Usually, it is a good practice to keep a copy of your dataset at the end of every junction (or major step), but due to the memory constraints of RStudio, we are going to keep updating the same dataset until the end.*

```{r}
yt_df <- clean_data(yt_df)
```

As you can see, the column names are standardized with this format (`small_case`) and we have also removed duplicate columns. Our dataset didn't have any duplicate values, for which we are very grateful.

#### 3.2. Tidy up the non-numerical columns

The non-numerical columns tend to have special characters (like \$, %, etc.) that might not be useful for classifying when a video will become trendy. Also, non-numerical columns might contain some ASCII or Unicode characters as well (such as `"\n"`) that might be interfering with the readability and analysis of the values. So, it's better to treat them by removing them from our dataset.

```{r udf_clean_char_cols}
clean_character_columns <- function(df) {   
  df <- df %>%     
    mutate(across(where(is.character), ~ {       
      cleaned_text <- str_squish(str_replace_all(., "\\p{C}+", ""))       
      ifelse(cleaned_text == "", NA, cleaned_text)     
    }))      
  return(df) 
}
```

```{r cleaning_unused_memory, include=FALSE}
gc()
```

```{r}
yt_df <- clean_character_columns(yt_df)
```

Here, the function `clean_character_columns` checks if the column is of type `character` and removes all the Unicode chars (such as `"\n","\r","\t"`). Finally, it trims the leading, trailing and extra spaces with the `str_squish(.)` function.

#### 3.3. Null Value Treatment

Now, it's time to treat the null values. First, let's define a `udf` to analyse the data and fetch the number of null values present in each column.

```{r udf_null_value}
analyze_columns <- function(df) {   
  calculate_mode <- function(x) {     
    unique_x <- unique(na.omit(x))     
    tabulated <- tabulate(match(x, unique_x))     
    unique_x[which.max(tabulated)]   
  }      
  # Create summary statistics for each column   
  summary_table <- data.frame(     
    `column_type` = sapply(df, typeof),     
    `number_of_nulls` = sapply(df, function(x) sum(is.na(x))),     
    `%_nulls` = sapply(df, function(x) round(sum(is.na(x)) / length(x) * 100, 2)),     
    `mean` = sapply(df, function(x) if (is.numeric(x)) round(mean(x, na.rm = TRUE), 2) else 0),     
    `median` = sapply(df, function(x) if (is.numeric(x)) round(median(x, na.rm = TRUE), 2) else 0),     
    `min` = sapply(df, function(x) if (is.numeric(x)) round(min(x, na.rm = TRUE), 2) else 0),     
    `max` = sapply(df, function(x) if (is.numeric(x)) round(max(x, na.rm = TRUE), 2) else 0),     
    `mode` = sapply(df, function(x) if (is.numeric(x) | is.character(x) | is.factor(x)) calculate_mode(x) else "N/A"), stringsAsFactors = FALSE     
  )      
  return(summary_table) 
}
```

```{r cleaning_unused_memory, include=FALSE}
gc()
```

```{r}
null_df <- analyze_columns(yt_df) 
print(null_df)
```

As visible in the table, only the column `video_tags` has null values in them—almost 1 million of them. We cannot treat them with any common values, since they are dependent on the video itself (i.e. each record). We can drop the rows that have null values, but we will decide this during our feature selection step to ensure we are not losing any valuable data.

#### 3.4. Dropping Unnecessary columns

We will do preliminary feature engineering by dropping unnecessary columns and columns that might not be useful based on the subject knowledge. Basic conditions for determining not-so-useful columns are:

1.  Columns with more than 75% null values
2.  Columns with only 1 unique values
3.  Entire column/row being null

Now, let's check the number of unique values and if there are columns with only a singular unique value we will drop them.

```{r udf_check_unique_values}
check_unique_values <- function(df) {   
  unique_counts <- sapply(df, function(col) length(unique(col)))      
  return(cbind(Unique_Values = unique_counts)) 
}
```

```{r}
unique_values_table <- check_unique_values(yt_df) 
print(unique_values_table)
```

Okay, there are no columns with a singular unique value so let's continue.

### 4. Export the Cleaned Data (Optional)

This function will export the cleaned data into a `.csv` file into the same path from where you have imported the dataset and with a suffix [`_cleaned`] to the original name.

```{r udf_export_dataset}
#export_data <- function(df, input_filepath){   
#  output_filepath <- sub(".csv","_cleaned.csv",input_filepath)   
#  write.csv(df, output_filepath, row.names = FALSE) 
}
```

```{r}
#export_data(yt_df, data_filepath)
```

```{r cleaning_unused_memory, include=FALSE}
# This block removes all the elements from your environment and cleans up unused memory from the session
# remove(list = ls())
#gc()
```

------------------------------------------------------------------------

## Exploratory Data Analysis

In this step of the analysis, the aim is to gain a deeper understanding of the data so we can identify obvious errors, missing data, recognize potential patterns, as well as spot outliers.

## 1. Univariate Analysis

#### Import Dataset (Optional)

```{r import_cleaned_dataset}
# Update this variable with the your own filepath 
# data_filepath <- "./dataset/trending_yt_videos_113_countries_cleaned.csv"

#Import dataset 
# yt_df <- read.csv(data_filepath)
```

The first step in EDA is "`Univariate analysis`" which - in a nutshell - means we will analyze each variable separately. This gives an in-depth understanding of all the variables and is an important step. To put it in car terms: ***without knowing what each screw and bolt does, you cannot possibly build a car or repair it.***

#### 1.1. Numerical Columns

Let's start by defining some `function(s)` that will take a dataframe as input, check for the type of column, and if its the intended type (e.g. `int` or `chr`) for that function, it will analyze that column and print the respective results. The `udf` will calculate the `mean, median, mode, skweness, kutosis, standard deviation,` and `quartile values` for the `numerical columns` in the dataset.

```{r udf_analyse_numerical_cols}
analyze_numerical <- function(df) {
  numerical_cols <- names(df)[sapply(df, is.numeric)]
  
  for (col in numerical_cols) {
    cat("\nColumn Name: ", col, "\n")
    print(summary(df[[col]]))
    cat("Skewness:", e1071::skewness(df[[col]], na.rm = TRUE), "\n")
    cat("Kurtosis:", e1071::kurtosis(df[[col]], na.rm = TRUE), "\n")
    print("-------------------------------------------------------")
    
    # Visualizations
    pic_1 <- ggplot(df, aes(x = .data[[col]])) + 
      geom_histogram(aes(y = ..density..), bins = 30, fill = "blue", alpha = 0.5) +
      geom_density(color = "red") +
      ggtitle(paste("Histogram of", col))
    
    pic_1 <- ggplot(df, aes(y = .data[[col]])) + 
      geom_boxplot(fill = "orange", alpha = 0.5) +
      ggtitle(paste("Boxplot of", col))
    
    print(pic_1)
    print(pic_1)
  }
}
```

Please be patient when running the below code chunk—it will take a few minutes. In the meantime, think of all the great visuals that it will give you!

```{r}
analyze_numerical(yt_df)
```

**Abnormalities Numerical Variables** 

*Skewness:* A prominent abnormality across several variables (`daily_rank`, `view_count`, `like_count`, and `comment_count`) is the strong right-skew in their distributions. This suggests that for these variables, most of the values are concentrated at the lower end with a few very large values pulling the mean to the right. This is common in marketing campaign data or data related to popularity/engagement if content is a hit. As mentioned before, these variables are typical KPIs used by industry professionals.

*Outliers:* The boxplots reveal the presence of outliers in `daily_movement`, `weekly_movement`, `view_count`, `like_count`, and `comment_count`. These outliers indicate extreme values that deviate significantly from the rest of the data. In the context of view, like, and comment counts, outliers might represent exceptionally popular or trendy content. 

*Central Tendency and Spread:* For `daily_movement` and `weekly_movement`, the distributions are centered around zero, suggesting that the movements are balanced around no net change. The spread, as indicated by the boxplots and density plots, shows the typical magnitude of these movements. 

**Insights Numerical Variables** 

1.  The right-skew in `view_count`, `like_count`, and `comment_count` suggests that a small proportion of content attracts a disproportionately large share of views, likes, and comments. This insight is crucial for content creators or platform managers, as it highlights the importance of identifying and potentially promoting content with the potential to become highly popular. 
2.  The distributions of `daily_movement` and `weekly_movement` being centered around zero might imply a certain level of stability in the underlying system, with fluctuations occurring around a central equilibrium. However, the outliers in these variables could indicate instances of significant shifts. These might be of most interest for our business problem to classify trendy videos.

For an more detailed overview of analysis per numerical variable and corresponding boxplot, please see the table below.

| Variable | Distribution | Range and Outliers | Density |
|----|----|----|----|
| `daily_rank` | The histogram of `daily_rank` shows a right-skewed distribution, indicating that most of the values are concentrated towards the lower end of the rank, with fewer occurrences of higher rank values. | The boxplot confirms the right-skewness and suggests a limited range of `daily_rank` values, mostly between 0 and 25. There don't appear to be significant outliers. | The density plot illustrates the skew more clearly, with a peak at the lower end and a tail extending to the right. |
| `daily_movement`  | The histogram of `daily_movement` shows a distribution centered around zero, with frequencies decreasing as you move away from zero in both positive and negative directions. | The boxplot shows the median `daily_movement` is around zero, with the interquartile range (IQR) indicating the spread of the central 50% of the data. The whiskers suggest a range from approximately -40 to +40, with a few potential outliers beyond these ranges. | The density plot shows a peak around zero, confirming the distribution's center, and tails off symmetrically, suggesting a distribution that might be close to normal but possibly with heavier tails. |
| `weekly_movement` | Similar to `daily_movement`, the histogram of `weekly_movement` is centered around zero, with a decrease in frequency as you move towards the extremes. | The boxplot indicates the median `weekly_movement` is approximately zero. The IQR represents the spread of the central 50%. The whiskers extend to around -40 and +40, with some outliers beyond these points. | The density plot shows a peak at zero and a symmetric decrease on both sides, resembling a normal distribution curve, but again, the tails might be heavier. |
| `view_count` | The `view_count` histogram is heavily right-skewed, indicating that most observations have lower view counts, and only a few have very high counts. | The boxplot illustrates the skew and shows that the majority of the data points are concentrated at the lower end, with a large number of outliers on the higher end of view_count.   | The density plot emphasizes the skew, with a high peak near zero and a long tail extending towards the higher values of view_count. |
| `like_count` | The histogram of `like_count` is also right-skewed, similar to `view_count`, with most counts concentrated at the lower end and fewer occurrences of very high `like_count` values. | The boxplot shows the concentration of data points at the lower end and the presence of many outliers on the higher end. | The density plot highlights the skew, with a sharp peak close to zero and a tail extending towards larger `like_count` values. |
| `comment_count` | The histogram of `comment_count` exhibits a strong right-skew, indicating that most observations have lower comment counts, with only a few having very high counts. | The boxplot illustrates that most values are near zero, with a substantial number of outliers on the higher end. | The density plot shows a high peak near zero and a long tail extending towards the higher `comment_count` values, demonstrating the skew. |

#### 1.2. Non-Numeric Columns

-   For `text data`, the function will calculate the most commonly appearing words in the `title` and `video_tags` columns in the `top 10%` of the videos.

-   Top 10% - based on `daily_rank`, `daily_movement` and `weekly_movement`. We merge the resultant vectors into one while removing the common words between the 2 techniques. We also correct the spelling using the `hunspell` library to ensure we have words which are meaningful.

-   Further in the analysis, we can use this result as a feature to train the model - to be one of the factors that determine the trendliness of the video.

```{r udf_tokenise_words}
stopwords_list <- stopwords("en")

tokenize <- function(text) {
  words <- unlist(str_extract_all(tolower(text), "\\b\\w+\\b"))
  words <- words[!(words %in% stopwords_list) & nchar(words) > 4 & !grepl("^\\d+$", words)]
  return(words)
}
```

```{r udf_top_words}
get_top_words <- function(df, ranking_type) {
  if (tolower(ranking_type) == "rank") {
    threshold <- quantile(df$daily_rank, 0.1, na.rm = TRUE)
    top_videos <- df %>% filter(daily_rank <= threshold)
  } else if (tolower(ranking_type) == "movement") {
    threshold_daily <- quantile(df$daily_movement, 0.1, na.rm = TRUE)
    threshold_weekly <- quantile(df$weekly_movement, 0.1, na.rm = TRUE)
    top_videos <- df %>% filter(daily_movement <= threshold_daily | weekly_movement <= threshold_weekly)
  } else {
    stop("Invalid ranking_type. Use 'rank' or 'movement'.")
  }
  
  # Ignore NA values
  top_videos <- top_videos %>% drop_na(title, video_tags)
  
  # Process title words
  title_words <- unlist(strsplit(top_videos$title, " ")) %>% tokenize()
  title_word_counts <- sort(table(title_words), decreasing = TRUE)
  
  # Process video tags words
  tag_words <- unlist(strsplit(top_videos$video_tags, " ")) %>% tokenize()
  tag_word_counts <- sort(table(tag_words), decreasing = TRUE)
  
  return(list(
    names(title_word_counts)[1:25],
    names(tag_word_counts)[1:25]
  ))
}
```

```{r udf_normalize_words}
normalize_word <- function(word) {
  word <- tolower(word)  # Convert to lowercase
  word <- gsub("[- ]", "", word)  # Remove hyphens and spaces
  return(word)
}
```

```{r udf_correct_spelling}
correct_spelling <- function(words) {
  words <- unique(sapply(words, normalize_word))  # Normalize words first
  corrected_words <- unique(unlist(sapply(words, function(word) {
    suggestions <- hunspell_suggest(word)
    if (length(suggestions) > 0) {
      return(suggestions[[1]][1])  # Pick the first suggestion
    } else {
      return(word)  # Keep original word if no suggestion available
    }
  })))
  return(sort(corrected_words))
}
```

```{r udf_merge_word_tuples}
merge_word_tuples <- function(tuple1, tuple2) {
  title_words_combined <- unique(c(tuple1[[1]], tuple2[[1]]))
  tag_words_combined <- unique(c(tuple1[[2]], tuple2[[2]]))
  
  return(list(
    correct_spelling(title_words_combined),
    correct_spelling(tag_words_combined)
  ))
}
```

```{r}
common_words_list_rank <- get_top_words(yt_df, "rank")
common_words_list_movement <- get_top_words(yt_df, "movement")


final_results <- merge_word_tuples(common_words_list_rank, common_words_list_movement)
final_title_words <- final_results[[1]]
final_tag_words <- final_results[[2]]

print(paste("Final Title:", length(final_title_words), toString(final_title_words)))
print(paste("Final Tags:", length(final_tag_words), toString(final_tag_words)))
```

Now, we have 2 lists of words that we can use to introduce a synthetic column that, in turn, can be used as a feature to train the model.

```{r cleaning_unused_memory, include=FALSE}
gc()
```

There are also a few columns that seem to be irrelevant in helping our model predict trendiness of a video. These are: `channel_name`, `country`, `description`, `thumbnail_url`, `kind`, `video_id`, `channel_id`, `language.`

The variable `language` had alot of empty values, and different type of values e.g.: "zh-TW", or "ja",or "en-US". There are 2280084 values "missing" which are instances in which the language of the video was not recognized. This is a lot so we decided not including this variable as it did not seem as important for predicting trendiness.

### 2. Bivariate Analysis \<Content TB Added\>

<!--# Shalakha, can you please update this? If I remember correctly, you had graphs related to understanding the relationship between age of the video and trendiness -->

### 3. Outlier Identification

Next, we will check for outliers in the numerical column of our dataset. We can do this in 2 different ways. First is visually using box plots and, second, is using the IQR, where we calculate the upper and lower boundary of the column and anything outside this boundary is an outlier. Now, we will just detect if there are any outliers using the second method. *[Note: The visual inspection of outliers will be carried out during the EDA].*

We start by defining a `udf` to give us the report on the outliers based on the IQR calculation of each numerical column.

```{r udf_detect_outliers}
detect_outliers <- function(df) {
  numeric_cols <- df %>% 
    select(where(is.numeric))
  outlier_summary <- data.frame(column = character(), lower_bound = numeric(), 
                                upper_bound = numeric(), num_outliers = integer(), 
                                iqr = numeric(), q1 = numeric(), q3 = numeric())
                                
  for (col in colnames(numeric_cols)) {
    q1 <- quantile(numeric_cols[[col]], 0.25, na.rm = TRUE)
    q3 <- quantile(numeric_cols[[col]], 0.75, na.rm = TRUE)
    iqr_value <- q3 - q1
    lower_limit <- q1 - 1.5 * iqr_value
    upper_limit <- q3 + 1.5 * iqr_value
    num_of_outliers <- sum(numeric_cols[[col]] < lower_limit | numeric_cols[[col]] > upper_limit, na.rm = TRUE)
    
    outlier_summary <- rbind(outlier_summary, data.frame(column = col, lower_bound = lower_limit, 
                                                         upper_bound = upper_limit, num_outliers = num_of_outliers, 
                                                         iqr = iqr_value, q1 = q1, q3 = q3))
  }
  return(outlier_summary)
}
```

```{r}
outlier_report <- detect_outliers(yt_df)
print(outlier_report)
```

Outliers are very important for us because we seek to identify trendy videos which are inherently outliers. From the outlier report results, we get a better idea of what characteristics the highest performing videos in this dataset possess. That said, we need to look carefully at this report because some of the numbers are not realistic (e.g. negative view, like, and comment counts). We will run a correlation analysis with the "desirable" outliers (trendy videos) based on these results and compare to the correlation analysis of the entire dataset.

### 4. Creating the Outlier Dataframe

Let's create our dataframe with "desirable" outliers — in other words, the most trendy videos. Our criteria for being a trendy video is as follows:

1.  **Like-to-view ratio is in the top 5th percentile**

    Rationale: Engagement ratios are highly significant for trendiness. The higher the like-to-view ratio, the better.

2.  **Comment-to-view ratio is in the top 5th percentile**

    Rationale: Engagement ratios are highly significant for trendiness. The higher the comment-to-view ratio, the better.

3.  **View count is in the top 25th percentile**

    Rationale: A trendy video has a large reach (high views).

4.  **Daily rank is between 1-10**

    Rationale: Based on the outlier report, `daily_rank` Q1 = 13. We are interested in outliers in that category and go one step further to take the top 10 rankings as data for trendiness as we are looking for very special videos.

5.  **Daily movement is \>= -4**

    Rationale: `daily_movement` Q1 = -9 and Q3 = 10. We acknowledge trendy videos can be displaced by other trendy videos. We allow for some modest, temporary displacement in the grand scheme of the dataset.

6.  **Weekly movement \>= 0**

    Rationale: We are looking for videos which improve their ranking over time. A positive weekly movement means that the video is becoming progressively more popular (trendier) over time.

Let's start by establishing the new variables expressed as ratios for criteria #1 and #2.

```{r establish like-to-view and comment-to-view ratios}
yt_df <- yt_df %>%
  mutate(ratio_like_view = like_count / view_count)

yt_df <- yt_df %>%
  mutate(ratio_comment_view = comment_count / view_count)
```

With those ratios established, we can now filter videos according to our defined criteria. Of course, this is a simpler process because the rest of the criteria's variables are already present in our main dataframe (criteria #3-6). Therefore, we can simply filter data by variable and add those rows to a new dataframe. We call this new dataframe `trendy_df_numeric` because in order to properly run the correlation analysis, we need data in numerical form, not character. There will be characters in this dataframe, but we will convert them in the next section when before we begin the correlation analysis.

```{r filter outlier videos to the df}
trendy_df_numeric <- yt_df %>% 
  filter(
    # Criteria 1
    ratio_like_view >= quantile(ratio_like_view, 0.05, na.rm = TRUE),
    # Criteria 2
    ratio_comment_view >= quantile(ratio_comment_view, 0.05, na.rm = TRUE),
    # Criteria 3
    view_count >= quantile(view_count, 0.75, na.rm = TRUE),
    # Criteria 4
    daily_rank >= 1 & daily_rank <= 10,
    # Criteria 5
    daily_movement >= -4,
    # Criteria 6
    weekly_movement >= 0
  )
```

Great! We now have a dataframe with videos trending according to our criteria.

### 5. Principal Component Analysis

We can now begin the PCA. First thing's first, though: we need to convert non-numeric columns to numeric. Just to confirm, let's identify which columns are non-numeric and need to be converted.

```{r identify non-numeric}
str(trendy_df_numeric)
```

We can see that the following columns are non-numeric.

1.  `title`
2.  `snapshot_date`
3.  `publish_date`
4.  `video_tags`

Because `snapshot_date` is the same for all videos, we can drop that column. We will also drop `video_tags` as that column will be explored in a separate section.

Let's convert `title` and `publish_date` to numeric:

```{r numeric conversion}
#title
trendy_df_numeric$title_numeric <- as.numeric(factor(trendy_df_numeric$title))

#publish_date
trendy_df_numeric$date_numeric <- as.numeric(format(ymd_hms(trendy_df_numeric$publish_date), "%Y%m%d"))
```

With these converted to numeric, we need also to remove the original character versions of these columns in addition to `snapshot_date` and `video_tags`.

```{r drop now irrelevant columns}
trendy_df_numeric <- trendy_df_numeric %>% select(-c(title, publish_date, video_tags, snapshot_date))
```

We now move to standardize our data in preparation for the PCA (Principal Component Analysis). Because the ratio column values are quite small, standardizing them causes them to become nulls and subsequently gives an error when we run the PCA. We've already identified the top 5% of each ratio and added them to our dataframe. Therefore, we can safely drop those columns. The like, comment, and view columns will remain.

```{r drop ratio columns}
trendy_df_numeric <- trendy_df_numeric %>% select(-c(ratio_like_view, ratio_comment_view))
```

Let's drop any nulls and double check that we are set on that front.

```{r drop null}
trendy_df_numeric <- trendy_df_numeric %>% drop_na()
colSums(is.na(trendy_df_numeric))
```

With no more nulls and all columns being numeric, relevant and of appropriate values, we can standardize our trendy dataset.

```{r standardize}
trendy_standardized <- scale(trendy_df_numeric[, !names(trendy_df_numeric) %in% "classe" ], center = TRUE, scale = TRUE)
summary(trendy_standardized)
```

The trendy dataset (`trendy_standardized`) is now ready for PCA.

Let's quickly prepare the full dataset for PCA so we can compare the results side-by-side. Since we will be dropping columns and converting others, let's create a new dataframe for us to work in specifically for the PCA.

```{r PCA df}
full_df_numeric <- yt_df
```

First, we convert the same columns to numeric: `title` and `publish_date`.

```{r numeric conversion}
#title
full_df_numeric$title_numeric <- as.numeric(factor(full_df_numeric$title))

#publish_date
full_df_numeric$date_numeric <- as.numeric(format(ymd_hms(full_df_numeric$publish_date), "%Y%m%d"))
```

We then drop the same columns so we have the same variables as `trendy_standardized`.

```{r drop columns to equalize}
full_df_numeric <- full_df_numeric %>% select(-c(title, publish_date, video_tags, snapshot_date, ratio_like_view, ratio_comment_view))
```

Let's check for nulls and drop any that arise.

```{r drop null}
colSums(is.na(full_df_numeric))
full_df_numeric <- full_df_numeric %>% drop_na()
colSums(is.na(full_df_numeric))
```

Nice. We are now in a position to standardize.

```{r standardize}
full_standardized <- scale(full_df_numeric[, !names(full_df_numeric) %in% "classe" ], center = TRUE, scale = TRUE)
```

Now, we have two standardized dataframes: `trendy_standardized` for the trendy/outliers and `full_standardized` for the full dataset. Now, the moment you have all been waiting for: the PCA!

```{r pca for trendy and full}
cat("\n====================== Principal Component Analysis of trendy_standardized =====================\n\n")
results_trendy <- prcomp(trendy_standardized)
summary(results_trendy)
cat("\n================================================================================================\n")

cat("\n====================== Principal Component Analysis of full_standardized =======================\n\n")
results_full <- prcomp(full_standardized)
summary(results_full)
cat("\n================================================================================================\n")
```

For both datasets, 6 principal components comprise \>91% of the cumulative variance. The `full_standardized` set reaches 90% slightly before `trendy_standardized`, however. Let's take a look at the component loadings for the PCAs for a more comprehensive view.

```{r pca identification}
cat("\n====================== Principal Component Loadings: trendy_standardized ===================\n\n")
print(results_trendy$rotation)
cat("\n============================================================================================\n")

cat("\n====================== Principal Component Loadings: full_standardized =====================\n\n")
print(results_full$rotation)
cat("\n============================================================================================\n")
```

Optional: sort variable loadings by absolute value for each PC. This will generate a large table but does make the loading distribution easier to scan through when looking for the most influential variables in each PC.
```{r organized pca loadings}
loadings_trendy <- results_trendy$rotation

cat("\n====================== Principal Component Loadings: trendy_standardized ===================\n")
for (pc in colnames(loadings_trendy)) {
  cat("\n=========================================")
  cat("\nTop contributors to", pc, ":\n")
  print(sort(abs(loadings_trendy[, pc]), decreasing = TRUE))
}

loadings_full <- results_full$rotation

cat("\n====================== Principal Component Loadings: full_standardized ===================\n")
for (pc in colnames(loadings_full)) {
  cat("\n=========================================")
  cat("\nTop contributors to", pc, ":\n")
  print(sort(abs(loadings_full[, pc]), decreasing = TRUE))
}
```

**PCA Insights:**

Taking a look at the variable contributions to each Principal Component, we deduce the following:

-   Like, comment, and view counts hold significant loadings in both datasets' PC1 indicating that audience engagement is a primary driver of variance (`trendy_standardized` PC1: 28% variance; `full_standardized` PC1: 29% variance).

-   `Daily_rank`, `daily_movement`, and `weekly_movement` have a stronger impact in the trendy dataset's PCA compared to that of the full dataset. Specifically, these metrics have a large influence in trendy dataset's PC2 and PC3 and relatively weaker influence in the full dataset PCs. While this is logical, these results do underscore the intertwined nature these metrics have with trendiness.

-   For the full dataset, earlier PCs are influenced more heavily by engagement metrics such as like, comment, and view counts. But in later PCs such as PC3 and PC4, metadata such as `title` and `date` play a larger role.

**Interpretations:**

-   The trendy dataset presents a more concentrated pattern as engagement and movement metrics were central in the PCs. This suggests a tighter profile of metrics to consider when aiming to predict virality.

-   In contrast, the full dataset's PC loadings were more varied which makes it challenging to predict virality based on this larger dataset.

------------------------------------------------------------------------

## Feature Engineering/Selection

We have completed our EDA and now we have understood the dataset completely. We can now create features (or update the existing columns) that will be fed as inputs to the model. There can be various methods of feature engineering such as selecting the important columns or creating new columns by merging (or using any other logic) the existing columns.

### 1. Convert Text Columns into Numerical Columns

First, we would start by converting the text columns (`title` and `video_tags`, in our case) into numerical columns, for better model predictability. We would be introducing the `scores` of the said columns using the following logic (`title_score` and `video_tags_score` columns). Take the value in the cell—say 1st cell of `title` or `video_tags` column ("*Highlights Real Madrid vs Barcelona 3-1...*")—and run the words separately through the list of words in `final_title_words` (or `final_tag_words`) and increase the count by 1 every time the word in the cell matches a word in the list. Example: The title "*Highlights Real Madrid vs Barcelona 3-1..."* will get a score of **3.**

```{r udf_calculate_text_score}
calculate_title_score <- function(title, word_list) {
  words <- tokenize(title)
  matches <- sapply(words, function(word) {
    any(grepl(paste0("\\b", word, "\\b"), word_list, ignore.case = TRUE))
  })
  # Ensure matches is a numeric vector (TRUE -> 1, FALSE -> 0)
  score <- sum(as.numeric(matches))
  return(as.integer(score))
}
```

```{r}
yt_df <- yt_df %>%
  mutate(title_score = vapply(title, calculate_title_score, FUN.VALUE = integer(1), word_list = final_title_words))

yt_df <- yt_df %>%
  mutate(video_tag_score = vapply(video_tags, calculate_title_score, FUN.VALUE = integer(1), word_list = final_tag_words))
```

This functions have added a column with the intended purpose. We will be using the synthetic columns inplace of the original text columns while training the model.

```{r export_feature_engg}
export_data <- function(df, input_filepath){   
  output_filepath <- sub(".csv","_feature_engg.csv",input_filepath)   
  write.csv(df, output_filepath, row.names = FALSE) 
}

export_data(yt_df, data_filepath)
```

#### Import Dataset (Optional)

```{r}
# Update this variable with the your own filepath
# data_filepath <- "./dataset/trending_yt_videos_113_countries_feature_engg.csv"

#Import dataset
# yt_df <- read.csv(data_filepath)
```

### 2. Creating other feature columns

Let's prep the data for that can be fed into the model. We will be creating a few additional columns such as `engagement_score`, `comment_score`, `video_age`, and the target column itself - `trending`. The formula used for the columns are as follows:

-   `engagement_score` = `like_count` / `view_count` *[Out of the total audience (people who view the video) how many people are liking the video?]*

-   `comment_score` = `comment_count` / `view_count` *[Out of the total audience (people who view the video) how many people are liking the video?]*

-   `video_age` = `snapshot_date` - `publish_date` *[Age of the video from the date it was uploaded to the date when this data was downloaded]*

```{r feature_cols_creation}
yt_df <- yt_df %>%
  mutate(
    engagement_score = ifelse(!is.na(view_count) & view_count > 0, 
                              round(like_count / view_count, 2), 0),
    comment_score = ifelse(!is.na(view_count) & view_count > 0, 
                           round(comment_count / view_count, 2), 0),
    video_age = as.numeric(difftime(snapshot_date, publish_date, units = "days"))
  )
```

To create the target variable (`trending`) - we start by calculating the `trend_score`. In simple words, the video that gets a `trend_score` of more than 80 (out of 100) will be considered as the positive class (`trending = 1`). The logic to calculate `trend_score` is as follows

-   `rank_score` -\> based on `daily_rank` column

-   `movement_score` -\> based on `daily_movement` column

-   `trend_score` = { 0.5 \* (`rank_score` + `movement_score`) \* 100 } + { 0.5 \* (`engagement_score`) \* 100 }

-   `trending` = 1 (when `trend_score` \> 80) or 0 (when `trend_score` \<= 80)

```{r udf_calc_trend_score}
calculate_trend_score <- function(df) {
  df <- df %>%
    mutate(
      # Calculate rank-based score (50% weightage)
      rank_score = ifelse(daily_rank < 10, (10 - daily_rank) / 10, 0),
      
      max_movement = max(daily_movement, na.rm = TRUE),
      movement_score = ifelse(daily_movement > 0 & max_movement > 0, 
                              daily_movement / max_movement, 0),
      
      weighted_rank_score = 0.5 * (rank_score + movement_score) * 100,  
      
      # Engagement-based score (50% weightage)
      weighted_engagement_score = ifelse(engagement_score > 0.1, 50, 0),
      
      # Compute final trend_score
      trend_score = (weighted_rank_score * 0.5) + weighted_engagement_score,
      
      # Determine if the video is trending
      trending = ifelse(trend_score > 80, 1, 0)
    ) %>%
    select(-rank_score, -max_movement, -movement_score, -weighted_rank_score, -weighted_engagement_score)

  return(df)
}
```

```{r}
yt_df <- calculate_trend_score(yt_df)
```

```{r export_model_training}
export_data <- function(df, input_filepath){   
  output_filepath <- sub(".csv","_model_training.csv",input_filepath)   
  write.csv(df, output_filepath, row.names = FALSE) 
}

export_data(yt_df, data_filepath)

gc()
```

## Model Training

Now that we have prepared our dataset (under the name `yt_df`), we can go ahead and select the appropriate models to classify wheter a video will become trendy. Since this is a binary classification problem (trendy vs not trendy), we selected models well-suited for this task.

First, we chose RandomForest. This ensemble method is known for its good performance on classification problems due to its ability to handle non-linear relationships, reduce overfitting through bagging, and provide insights via feature importance. It is also relative robust to noise in the data. We briefly considered Gradient Boosting, which builds models sequentially to correct errors from previous models. However, given our current goals of establishing an interpretable baseline, we decided to start with RandomForest due to its simplicity and faster training time

Additionally, we selected Logistic Regression. It is a widely used baseline for binary classification, it offers good interpretability. Logistic regression is similar to linear regression but instead it is used with a categorical response (trendy or not trendy).

### 1. RandomForest

#### Import Dataset (Optional)

```{r}
# Update this variable with the your own filepath 
# data_filepath <- "C:/Users/sabes/Desktop/Group Assignment/trending_yt_videos_113_countries_model_training.csv"

#Import dataset
yt_df <- read.csv(data_filepath)
```

#### 1.1 Prepping the data

In this section, we will be prepping the dataset even more, to be fed into the model. Like defining the `feature` and `target` variables, and `train-validation` split.

```{r feature_target_def}
# Defining the feature and target variables 
features <- c("title_score","video_tag_score","engagement_score","comment_score","video_age") 
target <- "trending"

# formula to be passed into the model(s)
formula <- as.formula(paste(target, "~", paste(features, collapse = "+")))

# Convert to a factor with valid labels
yt_df[[target]] <- factor(yt_df[[target]], 
                          levels = c(0, 1), 
                          labels = c("No", "Yes")
                          )
```

```{r train_val_split, warning=FALSE}
# spliting the data into train and validation sets 
split <- initial_split(yt_df, prop = 0.9, strata = target)
train_data <- training(split)
val_data <- testing(split)  

# prepping the data to be fed into the model 
X_train <- train_data[, features]
y_train <- train_data[, target]

X_val <- val_data[, features]
y_val <- val_data[, target]
```

First, we will define the `cross-validation` parameters and then proceed with `hyperparameter tuning`

```{r cross_val_def}
# cross validation
train_control <- trainControl(method="repeatedcv",
                              number=5,
                              repeats=3,
                              savePredictions = "final",    
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary,
                              sampling = "smote")
```

#### 1.2. Model Training

Now, that we have the data in the proper required format, we can start defining the model and train it.

```{r rf_model}
# (Optional) manual hyperparameter tuning 
# add tuneGrid = tune_grid into rf_model to use this
# tune_grid <- expand.grid(
#  mtry = c(2, 3, 4, 5),
#  splitrule = c("gini", "extratrees"),
#  min.node.size = c(1, 3, 5),
#  num.trees = c(100, 150, 200, 250, 300, 500, 750, 1000)
#  )

# random forest model
rf_model <- train(
  formula,
  data = train_data,
  method = "rf",
  metric = "ROC",
  trControl = train_control,
  tuneLength = 10, # automated hyperparameter tuning
  importance = TRUE
  )
```

#### 1.3. Exporting the model

Now, that we have our model trained with the training data (including cross-validation and hyperparameter tuning). Let's go ahead and check the model parameters. We will also be exporting the model into a `.RData` file, so that it can be used in the future.

```{r rf_model_params}
# full model details 
final_rf <- rf_model$finalModel
print(final_rf)

# saving the model for future use 
# save(rf_model, file = "rf_model.RData")
```

Now, let's check the importance of each feature that we have sent as input to the model in predicting each of the target class.

-   **No / Yes**: This column might represent whether the feature was used in the model (or some version of feature selection).

-   **MeanDecreaseAccuracy**: This measures how much **model accuracy** decreases when the feature is **excluded** from the model. The higher the value, the more important the feature is to the model's performance. If you exclude a feature with a high MeanDecreaseAccuracy, the accuracy of the model will drop significantly

-   **MeanDecreaseGini**: This measures how much the **Gini impurity** (a measure of node purity) is reduced when the feature is used to split the trees. Higher values indicate more important features, as they lead to splits that significantly reduce impurity in the decision trees. Features with high MeanDecreaseGini contribute more to the model's ability to separate classes.

#### 1.4. Performance Evaluation

First, let's start with understanding the importance of each feature as per the model.

```{r rf_feature_imp}
# to get feature importance 
print(final_rf$importance)

# plot the feature importance for better visualization  
importance_df <- data.frame(Feature = rownames(final_rf$importance),
                            Importance = final_rf$importance[, 2])

ggplot(importance_df,
       aes(x = reorder(Feature, Importance), y = Importance)) + 
  geom_bar(stat = "identity") + 
  coord_flip() +
  theme_minimal() + 
  labs(title = "Feature Importance in RF Model", x = "Feature", y = "Importance")
```

Here, we can see that `title_score`, `video_tag_score` are relatively less important than the `engagement_score`, `video_age` and `comment_score` in predicting the video's trendiness.

Now we are going to use the trained model to predict some unseen data. This will help us understand the actual performance of the model in the real-world where it will come across new data from time to time.

```{r rf_model_preds}
# Predict on your validation set
pred_class <- predict(final_rf, newdata = X_val)

# Confusion matrix 
confusionMatrix(pred_class, y_val)
```

\<Summary TB Added\>

### **2. Logistic Regression**

We will be repeating the `steps 1.2 to 1.4` for Logistic Regression.

*When running for the first time - make sure to enter "yes" in the terminal - to install the required packages*

```{r logit_model, message=FALSE, warning=FALSE}

# Train logistic regression model using features NOT in label creation
logit_model <- train(
  formula,
  data = train_data,
  method = "glm",
  family = "binomial",
  trControl = train_control,
  metric = "ROC"
)
```

```{r logit_model_params}
# full model details
print(logit_model$finalModel)
```

```{r logit_feature_imp}
# plot the feature importance for better visualization
importance <- varImp(logit_model, scale = TRUE)

importance_df <- importance$importance
importance_df$Feature <- rownames(importance_df)

importance_df %>%
  arrange(desc(Overall)) %>%
  ggplot(aes(x = reorder(Feature, Overall), y = Overall)) +
  geom_col(fill = "#2c7fb8") +
  coord_flip() +
  labs(title = "Feature Importance - Logistic Regression",
       x = "Feature",
       y = "Importance (Abs. Coefficient)") +
  theme_minimal()
```

Same as `Random Forest`, we can see that `video_tag_score` isrthe least important variable than the others in predicting the video's trendiness.

```{r logit_model_preds}
# Predict on your validation set
pred_class <- predict(logit_model, newdata = X_val)

# Confusion matrix 
confusionMatrix(pred_class, y_val, positive="Yes")
```

Based on accuracy both the models have higher values, but we need good precision (Pos Predictive Value) and Recall (sensitivity) for our use case, ie reducing the number of errors (both Type 1 and Type 2) to better understand the factors that are influencing the trendiness of a video.

So, based on these conditions we will be using the `Logistic Regression` model to predict the trending videos moving forward. As we can see from the importance of the features, a higher `engagement_score` (ie like to view count), `comment_score` (ie comment to view count), and `title_score` (ie the title with catchy words) can influence the video's trendiness. Other factors like `video_age`, and `video_tag_score` can influence it a bit, but better to focus on gaining more subscribers (to increase the `engagement_score` and `comment_score`) and using catchy words in the Title. Doing this will keep the users engaged and it will make your video to come out on top of the trending list.

## Conclusion

[conclusion and recommendations of Liam]

### Limitations

1.  **Data Quality:** First, we must start by acknowledging the quality of the data. The saying goes: “garbage in, garbage out.” In this sense, we are limited to the quality of data the author of the dataset has gathered. The limitation here is that with garbage data, we cannot derive great insights. Similarly, if the data is falsified, we cannot derive realistic insights. While we have verified a handful of the videos listed in the set, we make the assumption that the rest of the set is also real data.

2.  **Data Temporality:** We must also consider the temporality of the data. We started with a Kaggle dataset of trending videos downloaded on 20 February 2025. It is possible that videos trending in February 2025 do not reflect general conditions of trendiness during other periods of time. However, we believe it is reasonable to assume that our insights derived from this dataset can be safely applied to other times of the year.

3.  **Dataset Contents:** This dataset contains the top 50 trending videos across 113 countries but is not specifically limited to marketing-oriented videos. Some of these videos are marketing content, others are made by content creators whose videos are marketing, in essence, as they build upon their brand. In this sense, a potential limitation of this data is that the insights may not be applicable to marketing-oriented content due to the general nature of the dataset. We acknowledge this and believe that an understanding of trendiness itself is valuable for marketing professionals and that they will be able to use these insights to boost their marketing efforts.

4.  **PCA Columns:** For our PCA, we used a dataset with less dimensionality than the original. We decided to drop columns which did not have many unique values and/or could not be converted to be numeric. In doing this, we reduced our dimensionality pre-PCA to 8 columns. One limitation of this reduction is that we overlooked a column which turns out to be valuable.

5.  **R Literacy:** The R-savviness of the marketing professional may limit their ability to effectively leverage this machine learning model to their advantage. We strive to make our conclusion and application section as clear as possible for those who may be unfamiliar with R, there remains the risk of misinterpretation.

### **Further Steps for Improvement**

1.  **More Data:** The first step for further improvement will naturally be more data. We firmly believe in our models but testing sets of older data could provide valuable insights into how trendiness criteria have changed over time. This could entail varying user engagement and/or different content strategy. Additional datasets could deepen the understanding of trendiness by revealing larger trends which our singular dataset may not be able to capture.
2.  **YouTube Shorts:** Another area of improvement would be to look at YouTube Shorts—their product which competes with TikTok and Instagram Reels. Short-form video caters to a different customers than full-length YouTube videos and thus have different strategies. Notably, the Shorts are—as the name implies—short. This means the videos need to be engaging the user extremely frequently—about every 3 seconds. A separate model to uncover Shorts trendiness would directly support marketing professionals’ expansions into the short-form video, unlocking access new customers and improved brand awareness.

## References

1.  1,800 YouTube videos are posted every minute. [<https://seo.ai/blog/how-many-videos-are-on-youtube>] 
2.  Avg human attention span was 8 seconds in 2024. [<https://devrix.com/tutorial/user-attention-span/#:~:text=The%20average%20user%20attention%20span,8%20seconds%20in%20recent%20years>.] 

------------------------------------------------------------------------

| Group 12
| Keerthanapriya Kumaravel
| Liam Punselie
| Ryan Oosting
| Sabesanhari Rajamanikam
| Shalakha Deo
