---
title: "group12_ba_report"
output: html_document
---

# MOT122A - Business Analytics Group Assignment

## Overview

### Problem Statement

Capturing the audience's attention in the current digital landscape is a substantial challenge with the endless content at users' fingertips. 1,800 YouTube videos are posted every minute, amounting to a 360 hour increase in available content on the platform [[1](https://seo.ai/blog/how-many-videos-are-on-youtube)]. As a marketing professional, how will you ensure your video stands out? This challenge is further exacerbated by humans' ever-decreasing average attention span which, in 2024, was measured at just 8 seconds [[2](https://devrix.com/tutorial/user-attention-span/#:~:text=The%20average%20user%20attention%20span,8%20seconds%20in%20recent%20years)]. Attracting the audience's attention is challenging enough; sustaining it is yet another. Both aspects are critical for successful marketing campaigns which is why industry professionals such as content creators and product/service marketing teams need to know their audiences, know what keeps them interested, what drives them away, and what levers they can pull to increase their reach. This is where machine learning comes in.

### Task

Develop a machine learning model to classify a video trendiness given the video's metadata, engagement statistics, and temporality.

### Application

This machine learning model can be leveraged by marketing professionals to predict the likelihood that a given YouTube video will become trendy. This information informs tactical choices in the development process, enabling the professional to optimize video content and publishing strategies to increase the chance of the video becoming trendy. A trendy video subsequently increases the video's number of impressions which is a typical KPI (key performance indicator) among marketing teams. This in turn usually influences engagement metrics, another typical KPI among marketing campaigns and industry professionals.

### Dataset

We are using the dataset from Kaggle - [Trending Youtube Video Statistics (113 Countries)](https://www.kaggle.com/datasets/asaniczka/trending-youtube-videos-113-countries). Top 50 latest trending videos on YouTube across 113 countries. With **daily updates**, this dataset provides comprehensive information about the top trending videos, including daily rankings, movement trends, view counts, likes, comments, and more.

------------------------------------------------------------------------

## Data Cleaning

### 1. Importing the required libraries

Only use this cells to import all the required libraries. DO NOT import the libraries in any other cells below, it helps with maintaining the code properly and enhances the readability of the code.

```{r import_libraries, include=FALSE}
library(dplyr) 
library(tidyr) 
library(stringr) 
library(lubridate)
library(ggplot2)
library(textTinyR)
library(tm)
library(hunspell)
library(SnowballC)
library(factoextra)
library(C50)
library(gmodels)
library(Hmisc)
library(randomForest)
library(rsample)
```

Now that we have all the required libraries imported, let's dive into analyzing the dataset and train a ML model to classify the video as a Trending video or not. First let's start by importing the dataset and cleaning it.

#### Import Dataset

This cell imports the raw data. Since the data is too large for `GitHub` we can only use the data in our local system. So, please make sure you update the `filepath` with your own path before proceeding further in the notebook.

```{r import_dataset}
# Update this variable with the your own filepath
data_filepath <-"C:/Users/LRJPu/OneDrive - Delft University of Technology/Documenten/TU delft/MOT/Y1-Q3/BA/Assignment/BA_Group12/trending_yt_videos_113_countries.csv"

#Import dataset
yt_df <- read.csv(data_filepath)
```

> *Tip: You can run all the cells at once to get the final model (or) you can import different versions of the dataset at particular sections and run the remainder of the codes. The latter is suggested, because of RStudio's limited memory capacity. The places where you can import the already created dataset will be marked with `import dataset (optional)` text.*

### 2. Inspecting the raw data

We are just performing an initial check on the raw data. We do not manipulate any data points here. The main aim is to understand the raw data. First, we will check the structure of the data.

```{r str_dataset}
# Structure of the dataset 
str(yt_df)
```

As we can see, we have a lot of `chr` datatypes and a few of `int` data type. Next, we will check the summary statistics (like mean, median, min and max of the numerical columns and length, class, mode for non-numerical columns) *[Tip: Check `terminal` output for better readability]*

```{r summary_dataset}
# Summary statistics 
summary(yt_df)
```

After we got a glimpse, of what we have in hand in the raw data. Let's visualize it in the table format and soak in the beauty. *[Tip: Use the arrows to navigate to other columns]*

```{r head_dataset}
# First few rows 
head(yt_df)
```

Now, its time to understand how many rows and columns we have in the raw data. Trust us, this step is as important as every other steps. It might not seem like an great insight right now, but if and when we have to merge this dataset with another, the original dimensions will come in handy. And after data pre-processing, later on in this report, we will compare the final model training dataset with the raw data it as this is a good practice to do.

```{r dimensions_dataset}
# Dimensions of the dataset 
dim(yt_df)
```

```{r numerical vs non-numerical}
# Create a classification of columns as numerical or non-numerical
column_types <- sapply(yt_df, is.numeric)

# Extract numerical and non-numerical column names
numerical_columns <- names(column_types[column_types == TRUE])
non_numerical_columns <- names(column_types[column_types == FALSE])

# Print grouped column names
cat("Numerical Columns:\n")
cat(numerical_columns, sep = "\n")

cat("\nNon-Numerical Columns:\n")
cat(non_numerical_columns, sep = "\n")
```

With 18 columns not all of them will be relevant for predicting whether a YouTube video becomes trendy or not. But at first-sight the following columns are worth diving deeper into:

[daily_rank:]{.underline} This is the position a video has at a given day. The ranking ranges from 1-50, in which 1 is the highest rank and 50 the lowest. The ranking is done of all videos within a country.

[daily_movement:]{.underline} This shows how much movement in ranking a video had at a given day.

[weekly_moment:]{.underline} Similar to `daily_movement` but then movement measured within a singel week.

[view_count:]{.underline} The amount of views a video has had, which is one of the primary KPIs for marketing campaigns and content creators.

Most industry professionals are also very interested in the interaction between their content and audience and for this they keep track of engagement metrics. Our dataset also contains a few columns that are very suitable to be part of engagement metrics:

[like_count:]{.underline} This is the total likes a video has received. It is also an important KPI and might become part of an engagement metric variable later on in the analysis.

[comment_count:]{.underline} Similar to `like_count` but then done with regards to comments per video.

**Summary insights**

1.  We have 2,720,499 rows and 18 columns in our dataset

2.  12 `char` (non-numerical) columns and 6 `int` (numerical) columns

3.  The `int` columns seems to be properly formatted (i.e. without any `str` in between)

4.  At first-sight interesting columns for predicting trendiness are `daily_rank` , `daily_movement` ,`weekly_movement` , and `view_count` . For engagement metrics columns `like_count` and `comment_count` seem relevant.

### 3. Data Pre-Processing

In this step, we will clean the raw data as much as possible, so that it is Exploratory Data Analysis (EDA) ready. First we start with making sure all the column have appropriate datatypes (like `date` type values in `snapshot_date` column).

#### 3.1. Standardizing column names and removing duplicate values

Let's define a user-defined function (`udf`) to do the initial cleaning of the dataset. The function will:

1.  Standardizing the column names (lower_case)
2.  Dropping duplicate rows
3.  Dropping unnecessary columns based on business knowledge reasoning is given later in EDA (This step was originally supposed to be at the end of EDA, but due to RStudio's memory constraints we are performing it here) [`channel_name`, `country`, `description`, `thumbnail_url`, `kind`, `video_id`, `channel_id`, `language]`.

```{r udf_std_colnames}
clean_data <- function(df){      
  # Standardize column names   
  cleaned_data <- df %>% rename_all(tolower) %>% rename_all(str_replace_all, pattern = " ", replacement = "_")      
  # Remove duplicate rows   
  df <- df %>% distinct()      
  # Dropping unnecessary columns
  df <- df %>% select(-c(channel_name, country, description, thumbnail_url, kind, video_id, channel_id, langauge))
  
  return(df) 
}
```

*Usually, it is a good practice to keep a copy of your dataset at the end of every junction (or major step), but due to the memory constraints of RStudio, we are going to keep updating the same dataset until the end.*

```{r}
yt_df <- clean_data(yt_df)
```

As you can see, the column names are standardized with this format - `small_case` and we have also removed any duplicate columns. Our dataset didn't have any duplicate values, for which we are very grateful.

#### 3.2. Tidy up the non-numerical columns

The non-numerical columns tend to have special characters (like \$, %, etc.) that might not be useful for classifying when a video will become trendy or not. Also, non-numerical columns might contain some ASCII or Unicode characters as well (such as `"\n"`) that might be interfering with the readability and analysis of the values. So, its better to treat them by removing them from our dataset.

```{r udf_clean_char_cols}
clean_character_columns <- function(df) {   
  df <- df %>%     
    mutate(across(where(is.character), ~ {       
      cleaned_text <- str_squish(str_replace_all(., "\\p{C}+", ""))       
      ifelse(cleaned_text == "", NA, cleaned_text)     
    }))      
  return(df) 
}
```

```{r cleaning_unused_memory, include=FALSE}
gc()
```

```{r}
yt_df <- clean_character_columns(yt_df)
```

Here, the function `clean_character_columns` checks if the column is of type `character` and removes all the Unicode chars (such as `"\n","\r","\t"`). Finally, it trims the leading, trailing and extra spaces with the `str_squish(.)` function.

#### 3.3. Null value treatment

Now, its time to treat the null values. First, let's define a `udf` to analyse the data and fetch us how many null values are present in each column.

```{r udf_null_value}
analyze_columns <- function(df) {   
  calculate_mode <- function(x) {     
    unique_x <- unique(na.omit(x))     
    tabulated <- tabulate(match(x, unique_x))     
    unique_x[which.max(tabulated)]   
  }      
  # Create summary statistics for each column   
  summary_table <- data.frame(     
    `column_type` = sapply(df, typeof),     
    `number_of_nulls` = sapply(df, function(x) sum(is.na(x))),     
    `%_nulls` = sapply(df, function(x) round(sum(is.na(x)) / length(x) * 100, 2)),     
    `mean` = sapply(df, function(x) if (is.numeric(x)) round(mean(x, na.rm = TRUE), 2) else 0),     
    `median` = sapply(df, function(x) if (is.numeric(x)) round(median(x, na.rm = TRUE), 2) else 0),     
    `min` = sapply(df, function(x) if (is.numeric(x)) round(min(x, na.rm = TRUE), 2) else 0),     
    `max` = sapply(df, function(x) if (is.numeric(x)) round(max(x, na.rm = TRUE), 2) else 0),     
    `mode` = sapply(df, function(x) if (is.numeric(x) | is.character(x) | is.factor(x)) calculate_mode(x) else "N/A"), stringsAsFactors = FALSE     
  )      
  return(summary_table) 
}
```

```{r cleaning_unused_memory, include=FALSE}
gc()
```

```{r}
null_df <- analyze_columns(yt_df) 
print(null_df)
```

As visible in the table, only the column `video_tags` has null values in them. We cannot treat them with any common values, since they are dependent on the video itself (i.e. each record). We can drop the rows that have null values, but we will decide this during our feature selection step - to ensure we are not losing any valuable data.

#### 3.4. Dropping Unnecessary columns

We will do a preliminary feature engineering, by dropping unnecessary columns and the columns that might not be useful based on the subject knowledge. Some basic conditions for determining not-so-useful columns are:

1.  Columns with more than 75% null values
2.  Columns with only 1 unique values
3.  Entire column/row being null

Now let's check the number of unique values and if there are columns with only 1 unique value we will drop them.

```{r udf_check_unique_values}
check_unique_values <- function(df) {   
  unique_counts <- sapply(df, function(col) length(unique(col)))      
  return(cbind(Unique_Values = unique_counts)) 
}
```

```{r}
unique_values_table <- check_unique_values(yt_df) 
print(unique_values_table)
```

Okay, clearly there are no columns with only 1 unique value so let's continue.

### 4. Export the cleaned data

Finally, we will export the cleaned data. So, that we can use a different notebook to perform the EDA. For presenting our analysis we have included this EDA notebook by embedding it into the full report. This function will export the cleaned data into a `.csv` file into the same path from where you have imported the dataset and with a suffix [`_cleaned`] to the original name.

```{r udf_export_dataset}
export_data <- function(df, input_filepath){   
  output_filepath <- sub(".csv","_cleaned.csv",input_filepath)   
  write.csv(df, output_filepath, row.names = FALSE) 
}
```

```{r}
export_data(yt_df, data_filepath)
```

That brings us to the end of the data cleaning part. Any more cleaning, clustering, merging or separation of the dataset(s) will happen in the next step(s).

```{r cleaning_unused_memory, include=FALSE}
# This block removes all the elements from your environment and cleans up unused memory from the session
# remove(list = ls())
gc()
```

------------------------------------------------------------------------

## Exploratory Data Analysis

In this step of the analysis the aim is to gain a deeper understanding of the data so that we can identify obvious errors, missing data, recognize potential patterns, as well as spot outliers.

```{r import_cleaned_dataset}
# Update this variable with the your own filepath
# data_filepath <- "C:/Users/sabes/Desktop/Group Assignment/trending_yt_videos_113_countries_cleaned.csv"

#Import dataset
# yt_df <- read.csv(data_filepath)
```

### 1. Univariate Analysis

#### Import Dataset (Optional)

```{r}
# Update this variable with the your own filepath 
 data_filepath <- "C:/Users/LRJPu/OneDrive - Delft University of Technology/Documenten/TU delft/MOT/Y1-Q3/BA/Assignment/BA_Group12/trending_yt_videos_113_countries_cleaned.csv"  

# Import dataset 
 yt_df <- read.csv(data_filepath)
```

The first step in EDA is "`Univariate analysis`" - in a nutshell - meaning that we will analyse each variable separately. This helps us to have an in-depth understanding of all the variables. It is an important step because - to put it in terms of a car - ***without knowing what each screw and bolt does, you cannot possibly build a car or repair it.***

#### 1.1. Numerical & Categorical Columns

Let's start by defining some `function(s)` that will take a dataframe as input, check for the type of column and if its the intended type (eg `int` or `chr`) for that function, it will analyse that column and print the respective results.

-   For `numerical columns` the function will calculate the `mean, median, mode, skweness, kutosis, standard deviation,` and `quartile values`.

-   For `categorical columns` the function will check for the `frequency counts` and plots a `bar graph`

```{r udf_analyse_numerical_cols}
analyze_numerical <- function(df) {
  numerical_cols <- names(df)[sapply(df, is.numeric)]
  
  for (col in numerical_cols) {
    cat("\nColumn Name: ", col, "\n")
    print(summary(df[[col]]))
    cat("Skewness:", e1071::skewness(df[[col]], na.rm = TRUE), "\n")
    cat("Kurtosis:", e1071::kurtosis(df[[col]], na.rm = TRUE), "\n")
    print("-------------------------------------------------------")
    
    # Visualizations
    pic_1 <- ggplot(df, aes(x = .data[[col]])) + 
      geom_histogram(aes(y = ..density..), bins = 30, fill = "blue", alpha = 0.5) +
      geom_density(color = "red") +
      ggtitle(paste("Histogram of", col))
    
    pic_1 <- ggplot(df, aes(y = .data[[col]])) + 
      geom_boxplot(fill = "orange", alpha = 0.5) +
      ggtitle(paste("Boxplot of", col))
    
    print(pic_1)
    print(pic_1)
  }
}
```

```{r udf_analyse_cat_cols}
analyze_categorical <- function(df) {
  categorical_cols <- names(df)[sapply(df, is.character) | sapply(df, is.factor)]
  
  for (col in categorical_cols) {
    cat("\nColumn Name: ", col, "\n")
    print(table(df[[col]]))
    
    # Visualization
    pic <- ggplot(df, aes(x = .data[[col]])) + 
      geom_bar(fill = "blue", alpha = 0.5) +
      ggtitle(paste("Bar Plot of", col)) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    print(pic)
    print("-------------------------------------------------------")
  }
}
```

Please be patient when running the below code chunk it will take a few minutes, but in the mean time think of all the nice visuals that it will give you in return!

```{r}
analyze_numerical(yt_df)
```

```{r}
analyze_categorical(yt_df)
```

<!--# why does above function not work/prints every single non-numerical instance? -->

<!--# why do some mention histograms and other boxplots? I see only boxplots as output from analyze_categorical()? And everything seems plotted 2x per variable?-->

**Abnormalities numerical variables** 

*Skewness:* A prominent abnormality across several variables (`daily_rank`, `view_count`, `like_count`, and `comment_count`) is the strong right-skew in their distributions. This suggests that for these variables, most of the values are concentrated at the lower end, with a few very large values pulling the mean to the right. This is common in marketing campaign data or data related to popularity or engagement. As mentioned before, these variables are typical KPIs used by industry professionals.

*Outliers:* The boxplots reveal the presence of outliers in `daily_movement`, `weekly_movement`, `view_count`, `like_count`, and `comment_count`. These outliers indicate extreme values that deviate significantly from the rest of the data. In the context of view, like, and comment counts, outliers might represent exceptionally popular or trendy content. 

*Central Tendency and Spread:* For `daily_movement` and `weekly_movement`, the distributions are centered around zero, suggesting that the movements are balanced around no net change. The spread, as indicated by the boxplots and density plots, shows the typical magnitude of these movements. 

**Insights numerical variables** 

1.  The right-skew in `view_count`, `like_count`, and `comment_count` suggests that a small proportion of content attracts a disproportionately large share of views, likes, and comments. This insight is crucial for content creators or platform managers, as it highlights the importance of identifying and potentially promoting content with the potential to become highly popular. 

<!-- -->

2.  The distributions of `daily_movement` and `weekly_movement` being centered around zero might imply a certain level of stability in the underlying system, with fluctuations occurring around a central equilibrium. However, the outliers in these variables could indicate instances of significant shifts. These might be of most interest for our business problem to classify trendy videos.

For an more detailed overview of analysis per numerical variable and corresponding boxplot, see the table below.

| Variable | Distribution | Range and Outliers | Density |
|----------------|----------------------|-------------------|-----------------|
| `daily_rank` | The histogram of `daily_rank` shows a right-skewed distribution, indicating that most of the values are concentrated towards the lower end of the rank, with fewer occurrences of higher rank values. | The boxplot confirms the right-skewness and suggests a limited range of `daily_rank` values, mostly between 0 and 25. There don't appear to be significant outliers. | The density plot illustrates the skew more clearly, with a peak at the lower end and a tail extending to the right. |
| `daily_movement`  | The histogram of `daily_movement` shows a distribution centered around zero, with frequencies decreasing as you move away from zero in both positive and negative directions. | The boxplot shows the median `daily_movement` is around zero, with the interquartile range (IQR) indicating the spread of the central 50% of the data. The whiskers suggest a range from approximately -40 to +40, with a few potential outliers beyond these ranges. | The density plot shows a peak around zero, confirming the distribution's center, and tails off symmetrically, suggesting a distribution that might be close to normal but possibly with heavier tails. |
| `weekly_movement` | Similar to `daily_movement`, the histogram of `weekly_movement` is centered around zero, with a decrease in frequency as you move towards the extremes. | The boxplot indicates the median `weekly_movement` is approximately zero. The IQR represents the spread of the central 50%. The whiskers extend to around -40 and +40, with some outliers beyond these points. | The density plot shows a peak at zero and a symmetric decrease on both sides, resembling a normal distribution curve, but again, the tails might be heavier. |
| `view_count` | The `view_count` histogram is heavily right-skewed, indicating that most observations have lower view counts, and only a few have very high counts. | The boxplot illustrates the skew and shows that the majority of the data points are concentrated at the lower end, with a large number of outliers on the higher end of view_count.   | The density plot emphasizes the skew, with a high peak near zero and a long tail extending towards the higher values of view_count. |
| `like_count` | The histogram of `like_count` is also right-skewed, similar to `view_count`, with most counts concentrated at the lower end and fewer occurrences of very high `like_count` values. | The boxplot shows the concentration of data points at the lower end and the presence of many outliers on the higher end. | The density plot highlights the skew, with a sharp peak close to zero and a tail extending towards larger `like_count` values. |
| `comment_count` | The histogram of `comment_count` exhibits a strong right-skew, indicating that most observations have lower comment counts, with only a few having very high counts. | The boxplot illustrates that most values are near zero, with a substantial number of outliers on the higher end. | The density plot shows a high peak near zero and a long tail extending towards the higher `comment_count` values, demonstrating the skew. |

: Table overview of insights from boxplots numerical variables

#### 1.2. Non-Numeric Columns

-   For `text data` the function will calculate most commonly appearing words in the `title and video_tags` columns in the `top 10%` of the videos.

-   Top 10% - based on `daily_rank, daily_movement and weekly_movement.` And merging the resultant vectors into one, while removing the common words between the 2 techniques and correcting the spelling using `hunspell` library to ensure we have the words that are meaningful.

-   Further in the analysis, we can use this result as a feature to train the model - to be one of the factors that determine the trendliness of the video.

```{r udf_tokenise_words}
stopwords_list <- stopwords("en")

tokenize <- function(text) {
  words <- unlist(str_extract_all(tolower(text), "\\b\\w+\\b"))
  words <- words[!(words %in% stopwords_list) & nchar(words) > 4 & !grepl("^\\d+$", words)]
  return(words)
}
```

```{r udf_top_words}
get_top_words <- function(df, ranking_type) {
  if (tolower(ranking_type) == "rank") {
    threshold <- quantile(df$daily_rank, 0.1, na.rm = TRUE)
    top_videos <- df %>% filter(daily_rank <= threshold)
  } else if (tolower(ranking_type) == "movement") {
    threshold_daily <- quantile(df$daily_movement, 0.1, na.rm = TRUE)
    threshold_weekly <- quantile(df$weekly_movement, 0.1, na.rm = TRUE)
    top_videos <- df %>% filter(daily_movement <= threshold_daily | weekly_movement <= threshold_weekly)
  } else {
    stop("Invalid ranking_type. Use 'rank' or 'movement'.")
  }
  
  # Ignore NA values
  top_videos <- top_videos %>% drop_na(title, video_tags)
  
  # Process title words
  title_words <- unlist(strsplit(top_videos$title, " ")) %>% tokenize()
  title_word_counts <- sort(table(title_words), decreasing = TRUE)
  
  # Process video tags words
  tag_words <- unlist(strsplit(top_videos$video_tags, " ")) %>% tokenize()
  tag_word_counts <- sort(table(tag_words), decreasing = TRUE)
  
  return(list(
    names(title_word_counts)[1:25],
    names(tag_word_counts)[1:25]
  ))
}
```

```{r udf_normalize_words}
normalize_word <- function(word) {
  word <- tolower(word)  # Convert to lowercase
  word <- gsub("[- ]", "", word)  # Remove hyphens and spaces
  return(word)
}
```

```{r udf_correct_spelling}
correct_spelling <- function(words) {
  words <- unique(sapply(words, normalize_word))  # Normalize words first
  corrected_words <- unique(unlist(sapply(words, function(word) {
    suggestions <- hunspell_suggest(word)
    if (length(suggestions) > 0) {
      return(suggestions[[1]][1])  # Pick the first suggestion
    } else {
      return(word)  # Keep original word if no suggestion available
    }
  })))
  return(sort(corrected_words))
}
```

```{r udf_merge_word_tuples}
merge_word_tuples <- function(tuple1, tuple2) {
  title_words_combined <- unique(c(tuple1[[1]], tuple2[[1]]))
  tag_words_combined <- unique(c(tuple1[[2]], tuple2[[2]]))
  
  return(list(
    correct_spelling(title_words_combined),
    correct_spelling(tag_words_combined)
  ))
}
```

```{r}
common_words_list_rank <- get_top_words(yt_df, "rank")
common_words_list_movement <- get_top_words(yt_df, "movement")


final_results <- merge_word_tuples(common_words_list_rank, common_words_list_movement)
final_title_words <- final_results[[1]]
final_tag_words <- final_results[[2]]

print(paste("Final Title:", length(final_title_words), toString(final_title_words)))
print(paste("Final Tags:", length(final_tag_words), toString(final_tag_words)))
```

Now we have 2 lists of words that we can use to introduce a synthetic column that in turn can be used as a feature to train the model.

```{r cleaning_unused_memory, include=FALSE}
gc()
```

There are also a few columns that seem not as relevant to help our model predict trendiness of a video. These are: `channel_name`, `country`, `description`, `thumbnail_url`, `kind`, `video_id`, `channel_id`, `language.`

The variable `language` had alot of empty values, and different type of values e.g.: "zh-TW", or "ja",or "en-US". There are 2280084 values "missing" which are instances in which the language of the video was not recognized. This is a lot so we decided not including this variable as it did not seem as important for predicting trendiness.

<!--# elaborate! -->

### 2. Bivariate Analysis \<Content TB Added\>

<!--# Shalakha, can you please update this? If I remember correctly, you had graphs related to understanding the relationship between age of the video and trendiness -->

### 3. Outlier Identification

Next, we will check for outliers in the numerical column of our dataset. We can do this in 2 different ways. 1 - Visually using box plots and 2nd using the IQR, where we calculate the upper and lower boundary of the column (anything outside this boundary is an outlier). Now, we will just detect if there are any outliers using the 2nd method. *[Note: The visual inspection of outliers will be carried out during the EDA].*

We start by defining a `udf` to give us the report on the outliers based on the IQR calculation of each numerical column.

```{r udf_detect_outliers}
detect_outliers <- function(df) {
  numeric_cols <- df %>% 
    select(where(is.numeric))
  outlier_summary <- data.frame(column = character(), lower_bound = numeric(), 
                                upper_bound = numeric(), num_outliers = integer(), 
                                iqr = numeric(), q1 = numeric(), q3 = numeric())
                                
  for (col in colnames(numeric_cols)) {
    q1 <- quantile(numeric_cols[[col]], 0.25, na.rm = TRUE)
    q3 <- quantile(numeric_cols[[col]], 0.75, na.rm = TRUE)
    iqr_value <- q3 - q1
    lower_limit <- q1 - 1.5 * iqr_value
    upper_limit <- q3 + 1.5 * iqr_value
    num_of_outliers <- sum(numeric_cols[[col]] < lower_limit | numeric_cols[[col]] > upper_limit, na.rm = TRUE)
    
    outlier_summary <- rbind(outlier_summary, data.frame(column = col, lower_bound = lower_limit, 
                                                         upper_bound = upper_limit, num_outliers = num_of_outliers, 
                                                         iqr = iqr_value, q1 = q1, q3 = q3))
  }
  return(outlier_summary)
}
```

```{r}
outlier_report <- detect_outliers(yt_df)
print(outlier_report)
```

Outliers are very important for us because we seek to identify trendy videos which are inherently outliers. From the outlier report results, we get a better idea of what characteristics the highest performing videos in this dataset possess. That said, we need to look carefully at this report because some of the numbers are not realistic (e.g. negative view, like, and comment counts). We will run a correlation analysis with the "desirable" outliers based on these results and compare to the correlation analysis of the entire dataset.

### 4. Creating the Outlier Dataframe

Let's create our dataframe with "desirable" outliers — in other words, the most trendy videos.

<!--# add relationship between criteria to outlier data? and justification? -->

Our criteria for being a trendy video, or a "desirable" outlier, is as follows:

1.  Like-to-view ratio is in the top 5th percentile
2.  Comment-to-view ratio is in the top 5th percentile
3.  View count is in the top 25th percentile
4.  Daily rank is between 1-10
5.  Daily movement is \>= -4
6.  Weekly movement \>= 0

Since criteria #3-6 are based on existing variables in the dataset, let's start by establishing the new variables expressed in ratios for criteria #1 and #2.

```{r establish like-to-view and comment-to-view ratios}
yt_df <- yt_df %>%
  mutate(ratio_like_view = like_count / view_count)

yt_df <- yt_df %>%
  mutate(ratio_comment_view = comment_count / view_count)
```

With those ratios established, we can now filter videos according to our defined criteria. Of course, this is a simpler process because the rest of the criteria's variables are already present in our main dataframe. Therefore, we can simply filter data by variable and add rows to a new dataframe. We call this new dataframe "trendy_df_numeric" because in order to properly run the correlation analysis, we need data in numerical form, not character. There will still be characters in this dataframe, but we will convert them in the next section when we begin the correlation analysis.

```{r filter outlier videos to the df}
trendy_df_numeric <- yt_df %>% 
  filter(
    # Criteria 1
    ratio_like_view >= quantile(ratio_like_view, 0.05, na.rm = TRUE),
    # Criteria 2
    ratio_comment_view >= quantile(ratio_comment_view, 0.05, na.rm = TRUE),
    # Criteria 3
    view_count >= quantile(view_count, 0.75, na.rm = TRUE),
    # Criteria 4
    daily_rank >= 1 & daily_rank <= 10,
    # Criteria 5
    daily_movement >= -4,
    # Criteria 6
    weekly_movement >= 0
  )
```

Great. We now have a dataframe with videos trending according to our criteria.

### 5. Principal Component Analysis

Now, we can begin the PCA. First thing's first, though: we need to convert non-numeric columns to become numeric. Just to confirm, let's identify which columns are non-numeric and need to be converted.

```{r identify non-numeric}
str(trendy_df_numeric)
```

We can see that the following columns are non-numeric.

1.  title
2.  snapshot_date
3.  publish_date
4.  video_tags

Because snapshot_date is the same for all videos, we can drop that column. We will also drop video_tags as that column will be explored in a separate section.

Let's first convert title and publish_date:

```{r numeric conversion}
#title
trendy_df_numeric$title_numeric <- as.numeric(factor(trendy_df_numeric$title))

#publish_date
trendy_df_numeric$date_numeric <- as.numeric(format(ymd_hms(trendy_df_numeric$publish_date), "%Y%m%d"))
```

With these converted to numeric, we need also to remove the original character versions of these columns in addition to snapshot_date and video_tags.

```{r drop now irrelevant columns}
trendy_df_numeric <- trendy_df_numeric %>% select(-c(title, publish_date, video_tags, snapshot_date))
```

We now move to standardize our data in preparation for the PCA (Principal Component Analysis). Because the ratio column values are quite small, standardizing them causes them to become nulls and subsequently gives an error when we run the PCA. We've already identified the top 5% of each ratio and can therefore safely drop those columns. The like, comment, and view columns will remain.

```{r drop ratio columns}
trendy_df_numeric <- trendy_df_numeric %>% select(-c(ratio_like_view, ratio_comment_view))
```

Let's drop any nulls and double check that we are set on that front.

```{r drop null}
trendy_df_numeric <- trendy_df_numeric %>% drop_na()
colSums(is.na(trendy_df_numeric))
```

With all columns being numeric, relevant and of appropriate values, we can standardize our trendy dataset.

```{r standardize}
trendy_standardized <- scale(trendy_df_numeric[, !names(trendy_df_numeric) %in% "classe" ], center = TRUE, scale = TRUE)
summary(trendy_standardized)
```

The trendy dataset (trendy_standardized) is now ready for PCA.

Let's quickly prepare the full dataset for PCA so we can compare the results side-by-side. Since we will be dropping columns and converting others, let's create a new dataframe for us to work in specifically for the PCA.

```{r PCA df}
full_df_numeric <- yt_df
```

First, we convert the same columns to numeric: title and publish_date.

```{r numeric conversion}
#title
full_df_numeric$title_numeric <- as.numeric(factor(full_df_numeric$title))

#publish_date
full_df_numeric$date_numeric <- as.numeric(format(ymd_hms(full_df_numeric$publish_date), "%Y%m%d"))
```

We then drop the same columns so we have the same variables as the trendy dataframe.

```{r drop columns to equalize}
full_df_numeric <- full_df_numeric %>% select(-c(title, publish_date, video_tags, snapshot_date, ratio_like_view, ratio_comment_view))
```

Let's check for nulls and drop any that arise.

```{r drop null}
colSums(is.na(full_df_numeric))
full_df_numeric <- full_df_numeric %>% drop_na()
colSums(is.na(full_df_numeric))
```

Nice. We are now in a position to standardize.

```{r standardize}
full_standardized <- scale(full_df_numeric[, !names(full_df_numeric) %in% "classe" ], center = TRUE, scale = TRUE)
```

Amazing. Now, we have two standardized dataframes — one for the trendy/outliers and one for the full dataset. Now the moment we have all been waiting for (in this section): the PCA!

```{r pca for trendy and full}
cat("\n====================== Principal Component Analysis of trendy_standardized =====================\n\n")
results_trendy <- prcomp(trendy_standardized)
summary(results_trendy)
cat("\n================================================================================================\n")

cat("\n====================== Principal Component Analysis of full_standardized =======================\n\n")
results_full <- prcomp(full_standardized)
summary(results_full)
cat("\n================================================================================================\n")
```

For both datasets, 6 principal components comprise \>91% of the cumulative variance. The full_standardized set reaches 90% slightly before trendy_standardized, however. Let's take a look at the component loadings for the PCAs for a more comprehensive view.

```{r pca identification}
cat("\n====================== Principal Component Loadings: trendy_standardized ===================\n\n")
print(results_trendy$rotation)
cat("\n============================================================================================\n")

cat("\n====================== Principal Component Loadings: full_standardized =====================\n\n")
print(results_full$rotation)
cat("\n============================================================================================\n")
```

Optional: sort variable loadings by absolute value for each PC. This makes the loading chart easier to scan through when looking for the most influential variables in each PC.

```{r organized pca loadings}
loadings_trendy <- results_trendy$rotation

cat("\n====================== Principal Component Loadings: trendy_standardized ===================\n")
for (pc in colnames(loadings_trendy)) {
  cat("\n=========================================")
  cat("\nTop contributors to", pc, ":\n")
  print(sort(abs(loadings_trendy[, pc]), decreasing = TRUE))
}

loadings_full <- results_full$rotation

cat("\n====================== Principal Component Loadings: full_standardized ===================\n")
for (pc in colnames(loadings_full)) {
  cat("\n=========================================")
  cat("\nTop contributors to", pc, ":\n")
  print(sort(abs(loadings_full[, pc]), decreasing = TRUE))
}
```

**Insights PCA**

Taking a look at the variable contributions to each Principal Component, we deduce the following:

-   Like, comment, and view counts hold significant loadings in both datasets' PC1 indicating that audience engagement is a primary driver of variance (Trendy PC1: 28% variance; Full dataset PC1: 29% variance).

-   For the trendy dataset, daily rank, daily movement, and weekly movement have a stronger impact in the trendy dataset's PCA compared to that of the full dataset. Specifically, these metrics have a large influence in trendy dataset's PC2 and PC3 and relatively weaker influence in the full dataset PCs. While this is logical, these results do underscore the intertwined nature these metrics have with trendiness.

-   For the full dataset, earlier PCs are influenced more heavily by engagement metrics such as like, comment, and view counts. But in later PCs such as PC3 and PC4, metadata such as title and date play a larger role.

Interpretations:

-   The trendy dataset presents a more concentrated pattern as engagement and movement metrics were central in the PCs. This suggests a tighter profile of metrics to consider when aiming to predict virality.

-   In contrast, the full dataset's PC loadings were more varied which makes it challenging to predict virality based on this larger dataset.

------------------------------------------------------------------------

## Feature Engineering/Selection

### 1. Convert Text columns into Numerical columns

Logic of `title_score` and `video_tags_score` columns -\> Take the value in the cell (say 1st cell of title (or video_tags) column - "*Highlights Real Madrid vs Barcelona 3-1...*") and run the words separately through the list of words in `final_title_words` (or `final_tag_words`) and increase the count by 1 every time the word in the cell matches a word in the list. Example: The title "*Highlights Real Madrid vs Barcelona 3-1..."* will get a score of **3.**

```{r udf_calculate_text_score}
calculate_title_score <- function(title, word_list) {
  words <- tokenize(title)
  matches <- sapply(words, function(word) {
    any(grepl(paste0("\\b", word, "\\b"), word_list, ignore.case = TRUE))
  })
  # Ensure matches is a numeric vector (TRUE -> 1, FALSE -> 0)
  score <- sum(as.numeric(matches))
  return(as.integer(score))
}
```

```{r}
yt_df <- yt_df %>%
  mutate(title_score = vapply(title, calculate_title_score, FUN.VALUE = integer(1), word_list = final_title_words))

yt_df <- yt_df %>%
  mutate(video_tag_score = vapply(video_tags, calculate_title_score, FUN.VALUE = integer(1), word_list = final_tag_words))
```

This functions have added a column with the intended purpose. Let's try to feed that into the model and see if its improving the performance.

```{r export_feature_engg}
export_data <- function(df, input_filepath){   
  output_filepath <- sub(".csv","_feature_engg.csv",input_filepath)   
  write.csv(df, output_filepath, row.names = FALSE) 
}

export_data(yt_df, data_filepath)
```

#### Import Dataset (Optional)

```{r}
# Update this variable with the your own filepath
# data_filepath <- "C:/Users/sabes/Desktop/Group Assignment/trending_yt_videos_113_countries_feature_engg.csv"

#Import dataset
# yt_df <- read.csv(data_filepath)
```

Let's prep the data for that can be fed into the model.

```{r}
yt_df <- yt_df %>%
  mutate(
    engagement_score = ifelse(!is.na(view_count) & view_count > 0, 
                              round(like_count / view_count, 2), 0),
    comment_score = ifelse(!is.na(view_count) & view_count > 0, 
                           round(comment_count / view_count, 2), 0),
    video_age = as.numeric(difftime(snapshot_date, publish_date, units = "days"))
  )
```

```{r}
calculate_trend_score <- function(df) {
  df <- df %>%
    mutate(
      # Calculate rank-based score (50% weightage)
      rank_score = ifelse(daily_rank < 10, (10 - daily_rank) / 10, 0),
      
      max_movement = max(daily_movement, na.rm = TRUE),
      movement_score = ifelse(daily_movement > 0 & max_movement > 0, 
                              daily_movement / max_movement, 0),
      
      weighted_rank_score = 0.5 * (rank_score + movement_score) * 100,  
      
      # Engagement-based score (50% weightage)
      weighted_engagement_score = ifelse(engagement_score > 0.1, 50, 0),
      
      # Compute final trend_score
      trend_score = (weighted_rank_score * 0.5) + weighted_engagement_score,
      
      # Determine if the video is trending
      trending = ifelse(trend_score > 80, 1, 0)
    ) %>%
    select(-rank_score, -max_movement, -movement_score, -weighted_rank_score, -weighted_engagement_score)

  return(df)
}
```

```{r}
yt_df <- calculate_trend_score(yt_df)
```

```{r export_model_training}
export_data <- function(df, input_filepath){   
  output_filepath <- sub(".csv","_model_training.csv",input_filepath)   
  write.csv(df, output_filepath, row.names = FALSE) 
}

export_data(yt_df, data_filepath)

gc()
```

## Model Training

Now that we have prepared our dataset (under the name `yt_df`), we can go ahead and select the appropriate models to classify wheter a video will become trendy. Since this is a binary classification problem (trendy vs not trendy), we selected models well-suited for this task.

First, we chose RandomForest. This ensemble method is known for its good performance on classification problems due to its ability to handle non-linear relationships, reduce overfitting through bagging, and provide insights via feature importance. It is also relative robust to noise in the data. We briefly considered Gradient Boosting, which builds models sequentially to correct errors from previous models. However, given our current goals of establishing an interpretable baseline, we decided to start with RandomForest due to its simplicity and faster training time

Additionally, we selected Logistic Regression. It is a widely used baseline for binary classification, it offers good interpretability. Logistic regression is similar to linear regression but instead it is used with a categorical response (trendy or not trendy).

### Import Dataset (Optional)

```{r}
# Update this variable with the your own filepath
# data_filepath <- "<update here>/trending_yt_videos_113_countries_model_training.csv"

#Import dataset
# yt_df <- read.csv(data_filepath)

```

### 1. RandomForest

### **2. Boosting**

```{r model_boosting}
# Set parameters
iterations <- 15 # how many trees do you want to estimate?
randomize <- TRUE # use a randomly selected set of predictor variables?
min_randomize <- 2 # if randomize is set to TRUE, how many predictor variables do you want to sample (min = 2, max = 5)
proportion <- 0.7 # train test split

# Split dataframe in training and testing set



split <- rsample::initial_split(yt_df, prop = proportion)
training <- training(split)
testing <- testing(split)

target_var <- "daily_rank"  # Change this to match predictor variable later on!

# Boosting with C5.0
if (randomize) {
    predictors <- setdiff(names(training), target_var)  # Exclude target variable
    sampled_predictors <- sample(predictors, sample(min_randomize:length(predictors), 1))
    
    formula <- as.formula(paste(target_var, "~", paste(sampled_predictors, collapse = " + ")))
    model <- C5.0(formula, data = training, trials = iterations)  # Boosting with trials
} else {
    model <- C5.0(as.formula(paste(target_var, "~ .")), data = training, trials = iterations)
}

# Make predictions
predictions <- predict(model, testing)

# Add predictions to the testing set
testing$Predicted <- predictions

# Check model performance
conf_matrix <- table(testing[[target_var]], testing$Predicted)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print accuracy
print(paste("Model Accuracy:", round(accuracy * 100, 2), "%"))
```

## References

1.  1,800 YouTube videos are posted every minute. [<https://seo.ai/blog/how-many-videos-are-on-youtube>] 

<!-- -->

2.  Avg human attention span was 8 seconds in 2024. [<https://devrix.com/tutorial/user-attention-span/#:~:text=The%20average%20user%20attention%20span,8%20seconds%20in%20recent%20years>.] 

------------------------------------------------------------------------

| Group 12
| Keerthanapriya Kumaravel
| Liam Punselie
| Ryan Oosting
| Sabesanhari Rajamanikam
| Shalakha Deo
